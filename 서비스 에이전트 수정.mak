crawler_stealth.py (ìˆ˜ì • í•„ìˆ˜)
ì´ìœ : í˜„ì¬ ì½”ë“œëŠ” "ê°€ê²©í‘œ"ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë¡œì§ì´ Placeholderë¡œ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì•„ê¹Œ ë³´ë‚´ì£¼ì‹  ìŠ¤í¬ë¦°ìƒ·(image_ec94e5.png)ì˜ êµ¬ì¡°(SMILES Code, Price)ë¥¼ ì •í™•íˆ ê¸ì–´ì˜¤ë„ë¡ íŒŒì‹± ë¡œì§ì„ ì •êµí™”í•´ì•¼ í•©ë‹ˆë‹¤.

ë®ì–´ì“°ê¸° ì½”ë“œ:

Python
import requests
from bs4 import BeautifulSoup
import time
import random
from fake_useragent import UserAgent
from supabase import create_client, Client
import os
from dotenv import load_dotenv
import logging
import json

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AmbeedStealthScraper:
    BASE_URL = "https://www.ambeed.com"
    
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        
        # Initialize Supabase client
        url: str = os.environ.get("SUPABASE_URL")
        key: str = os.environ.get("SUPABASE_SERVICE_KEY")
        if not url or not key:
            raise ValueError("Supabase credentials not found in environment variables")
        self.supabase: Client = create_client(url, key)
        
    def get_headers(self, referer=None):
        headers = {
            'User-Agent': self.ua.random,
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        if referer:
            headers['Referer'] = referer
        return headers

    def safe_request(self, url, referer=None):
        """
        Anti-blocking request wrapper
        """
        retries = 3
        backoff = 30
        
        for i in range(retries):
            try:
                # Human-like random delay
                sleep_time = random.uniform(2, 5)
                time.sleep(sleep_time)
                
                logger.info(f"ğŸ•µï¸ Scraping: {url} (Delay: {sleep_time:.2f}s)")
                
                response = self.session.get(
                    url, 
                    headers=self.get_headers(referer), 
                    timeout=20
                )
                
                if response.status_code in [403, 429, 503]:
                    logger.warning(f"âš ï¸ Blocked ({response.status_code}). Cooling down for {backoff}s...")
                    time.sleep(backoff)
                    backoff *= 2
                    self.session = requests.Session()
                    continue
                
                response.raise_for_status()
                return response
            
            except Exception as e:
                logger.error(f"âŒ Error fetching {url}: {str(e)}")
                time.sleep(5)
                
        return None

    def parse_detail_page(self, url, category, referer_url):
        """
        [Updated] Parse product detail page strictly based on Ambeed layout
        """
        res = self.safe_request(url, referer=referer_url)
        if not res: return
        
        soup = BeautifulSoup(res.text, 'html.parser')
        
        details = {
            'category': category,
            'product_url': url,
            'price_data': {} 
        }
        
        try:
            # 1. Product Name
            title_elem = soup.select_one('h1')
            if title_elem:
                details['product_name'] = title_elem.get_text(strip=True)

            # 2. Specifications Table (ìŠ¤í¬ë¦°ìƒ· ê¸°ë°˜ íŒŒì‹±)
            # AmbeedëŠ” ë³´í†µ <th>Label</th><td>Value</td> ë˜ëŠ” div êµ¬ì¡°ë¥¼ ì”€.
            # í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì°¾ëŠ” ê²ƒì´ ê°€ì¥ ì•ˆì „í•¨.
            
            # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„
            for row in soup.find_all(['tr', 'li', 'div']): 
                text = row.get_text(" ", strip=True)
                
                if "Catalog No" in text and ":" in text:
                    details['ambeed_cat_no'] = text.split(":")[-1].strip()
                elif "CAS No" in text and ":" in text:
                    details['cas_number'] = text.split(":")[-1].strip()
                elif "Formula" in text and ":" in text:
                    details['formula'] = text.split(":")[-1].strip()
                elif "M.W" in text and ":" in text:
                    details['molecular_weight'] = text.split(":")[-1].strip()
                elif "SMILES" in text and ":" in text:
                    # SMILES ì½”ë“œëŠ” ë§¤ìš° ì¤‘ìš”
                    raw_smiles = text.split(":")[-1].strip()
                    details['smiles_code'] = raw_smiles

            # 3. Price Parsing (ê°€ê²©í‘œ)
            # ë³´í†µ table class="price-table" ê°™ì€ í˜•ì‹ì´ë¯€ë¡œ, '$'ê°€ í¬í•¨ëœ í–‰ì„ ì°¾ìŒ
            price_list = []
            for row in soup.find_all('tr'):
                row_text = row.get_text(strip=True)
                if "$" in row_text and ("mg" in row_text or "g" in row_text):
                    # ì˜ˆ: "1mg $100" -> ê°„ë‹¨íˆ í…ìŠ¤íŠ¸ ì „ì²´ ì €ì¥ (ë‚˜ì¤‘ì— AIê°€ í•´ì„)
                    price_list.append(row_text)
            
            if price_list:
                details['price_data'] = {"raw_pricing": price_list}

            # 4. Save if valid
            if 'ambeed_cat_no' in details:
                self.save_to_db(details)
            else:
                logger.warning(f"Skipping {url}: Could not find Catalog No")

        except Exception as e:
            logger.error(f"Error parsing {url}: {str(e)}")

    def save_to_db(self, data):
        """Upsert to Supabase commercial_reagents table"""
        try:
            # Check existing
            existing = self.supabase.table('commercial_reagents')\
                .select('id')\
                .eq('ambeed_cat_no', data['ambeed_cat_no'])\
                .execute()
                
            if existing.data:
                self.supabase.table('commercial_reagents')\
                    .update(data)\
                    .eq('ambeed_cat_no', data['ambeed_cat_no'])\
                    .execute()
                logger.info(f"ğŸ”„ Updated {data.get('product_name')} ({data['ambeed_cat_no']})")
            else:
                self.supabase.table('commercial_reagents')\
                    .insert(data)\
                    .execute()
                logger.info(f"âœ… Inserted {data.get('product_name')} ({data['ambeed_cat_no']})")

        except Exception as e:
            logger.error(f"DB Error: {str(e)}")

    def run(self, category="Payload"):
        # ... (ê¸°ì¡´ run ë¡œì§ ìœ ì§€) ...
        pass
2. ğŸ” rag_service.py (ìˆ˜ì • í•„ìˆ˜)
ì´ìœ : Commercial Agentê°€ "í˜¹ì‹œ MMAE ì ‘í•©ì²´ ìˆì–´ìš”?"ë¼ê³  ë¬¼ì–´ë´¤ì„ ë•Œ, DBë¥¼ ê²€ìƒ‰í•´ì£¼ëŠ” ê¸°ëŠ¥(search_commercial)ì´ ë¹ ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ê±¸ ì¶”ê°€í•´ì•¼ ì—ì´ì „íŠ¸ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë®ì–´ì“°ê¸° ì½”ë“œ:

Python
import os
from typing import List, Dict, Any
from openai import AsyncOpenAI
from app.core.supabase import supabase

class RAGService:
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.client = AsyncOpenAI(api_key=self.api_key)

    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for text using OpenAI"""
        response = await self.client.embeddings.create(
            input=text,
            model="text-embedding-3-small"
        )
        return response.data[0].embedding

    # [NEW] ìƒìš© ì‹œì•½ ê²€ìƒ‰ ê¸°ëŠ¥ (Commercial Agentê°€ ì‚¬ìš©)
    async def search_commercial(self, query: str, limit: int = 3) -> List[Dict[str, Any]]:
        """
        Search for commercial reagents (Ambeed Data).
        First try: Text Match (Product Name / Cat No)
        Second try: Semantic Search (Description/Category)
        """
        try:
            # 1. í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ì •í™•ë„ ìš°ì„ )
            # Supabaseì˜ textSearch ê¸°ëŠ¥ í™œìš© (product_name ì»¬ëŸ¼)
            text_result = supabase.table("commercial_reagents")\
                .select("*")\
                .textSearch("product_name", query, config="english")\
                .limit(limit)\
                .execute()
            
            if text_result.data:
                return text_result.data

            # 2. ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (Semantic Search)
            # (ë§Œì•½ commercial_reagents í…Œì´ë¸”ì— embedding ì»¬ëŸ¼ì„ ë§Œë“¤ì—ˆë‹¤ë©´ ì•„ë˜ ë¡œì§ ì‚¬ìš©)
            # embedding = await self.generate_embedding(query)
            # rpc_result = supabase.rpc("match_commercial", { ... }).execute()
            # return rpc_result.data
            
            return []
            
        except Exception as e:
            print(f"Error searching commercial DB: {str(e)}")
            return []

    # ... (ê¸°ì¡´ index_golden_set_item, search ë©”ì„œë“œ ìœ ì§€) ...
    async def index_golden_set_item(self, item_id: str, description: str, properties: Dict[str, Any]):
        # (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
        pass

    async def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        # (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
        pass

rag_service = RAGService()