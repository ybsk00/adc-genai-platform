1. ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°: "ë„ì„œê´€ê³¼ ë§ˆíŠ¸ë¥¼ ë¶„ë¦¬í•˜ì„¸ìš”"
ë°”ì´ì˜¤ë©(Creative Biolabs ë“±)ê³¼ ì»¤ë¨¸ì…œDB(Ambeed)ëŠ” **'ì‡¼í•‘'**ì´ ëª©ì ì´ê³ , ê³¨ë“ ì…‹(ClinicalTrials)ì€ **'ê³µë¶€'**ê°€ ëª©ì ì…ë‹ˆë‹¤.

commercial_reagents (ìƒìš© ì‹œì•½ í…Œì´ë¸”):

ì„±ê²©: êµ¬ë§¤ ê°€ëŠ¥, ê°€ê²©í‘œ ìˆìŒ, ì¬ê³  ìˆìŒ.

ë°ì´í„°: Ambeed(Payload/Linker), Creative Biolabs(Antibody).

í•„ë“œ: catalog_no, price, supplier, uniprot_id.

golden_set_library (ê³¨ë“ ì…‹ í…Œì´ë¸”):

ì„±ê²©: ì—°êµ¬ ë ˆí¼ëŸ°ìŠ¤, ì„±ê³µ/ì‹¤íŒ¨ ê¸°ë¡, êµ¬ë§¤ ë¶ˆê°€(ì™„ì œí’ˆ).

ë°ì´í„°: ClinicalTrials.gov, OpenFDA.

í•„ë“œ: nct_id, outcome, failure_reason, smiles_code (ì°¸ê³ ìš©).

ê²°ì •: í…Œì´ë¸”ì„ ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³ , ë‚˜ì¤‘ì— AIê°€ í•„ìš”í•  ë•Œë§Œ ì–‘ìª½ì„ ë‹¤ ì¡°íšŒí•´ì„œ ë§¤ì¹­(Mapping)í•˜ëŠ” ê²ƒì´ ì •ì„ì…ë‹ˆë‹¤.

ğŸ­ 2. ëŒ€ìš©ëŸ‰ ë¤í”„ ì²˜ë¦¬ & Gemini 2.0 Flash í™œìš© ì „ëµ
ìˆ˜ì‹­ë§Œ ê±´ì˜ ë°ì´í„°ë¥¼ ë‹¤ Geminiì—ê²Œ ë¬¼ì–´ë³´ë©´ ë¹„ìš©ì´ ë§ì´ ë“­ë‹ˆë‹¤. **[ê¹”ë•Œê¸° ì „ëµ]**ì„ ì¨ì•¼ í•©ë‹ˆë‹¤.

Level 1 (Python Filter): ë¤í”„ íŒŒì¼ì—ì„œ "ADC", "Antibody", "Conjugate" í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì¦‰ì‹œ ë²„ë¦¼ (Drop).

Level 2 (Gemini 2.0 Flash): ë‚¨ì€ ë°ì´í„° ì¤‘ "ì´ê²Œ ì§„ì§œ í•­ì•”ì œ ADC ë§ì•„?"ë¥¼ íŒë‹¨. (ë°°ì¹˜ ì²˜ë¦¬)

Level 3 (DB Insert): ê²€ì¦ëœ ê²ƒë§Œ ì €ì¥.

ğŸ§ª 3. SMILES ì½”ë“œ ì±„ìš°ê¸° (ì•ˆì „í•œ ë°°ì¹˜ ì „ëµ)
í•œ ë²ˆì— 1ë§Œ ê°œë¥¼ ëŒë¦¬ë©´ ì„œë²„ê°€ ì£½ìŠµë‹ˆë‹¤. [ì›Œì»¤ ë£¨í”„(Worker Loop)] ë°©ì‹ì´ ë‹µì…ë‹ˆë‹¤.

Select: smiles_codeê°€ ë¹„ì–´ìˆëŠ”(NULL) ë°ì´í„° 50ê°œë§Œ ê°€ì ¸ì˜´.

Process: í•˜ë‚˜ì”© PubChem API ì¡°íšŒ (ì—†ìœ¼ë©´ Geminiì—ê²Œ PubChem CID ê²€ìƒ‰ ìš”ì²­).

Update: DB ì—…ë°ì´íŠ¸.

Sleep: 2ì´ˆ íœ´ì‹ (API ì°¨ë‹¨ ë°©ì§€).

Repeat: ë‹¤ì‹œ 1ë²ˆìœ¼ë¡œ ëŒì•„ê°€ì„œ ë¬´í•œ ë°˜ë³µ.

ğŸ“¨ ê°œë°œìì—ê²Œ ì „ë‹¬í•  ìµœì¢… ì„¤ê³„ì„œ (INTELLIGENT_PIPELINE_SPEC.md)
ì´ ë‚´ìš©ì„ ê°œë°œìì—ê²Œ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.

ğŸ—ï¸ Intelligent Data Pipeline Specification
Goal: Separate commercial/reference data, implement bulk import with AI filtering, and safely enrich chemical data.

1. Database Schema Separation
Strictly separate the inventory data from reference data.

A. commercial_reagents (The Store)
Source: Ambeed, Creative Biolabs.

Purpose: Quoting, purchasing, ingredient selection.

Key Fields: id, supplier ('Ambeed', 'Creative'), catalog_no, name, price_data (JSON), structure_info (SMILES for payload, UniProt for mAb).

B. golden_set_library (The Library)
Source: ClinicalTrials.gov (Bulk), OpenFDA.

Purpose: Benchmarking, success/failure analysis.

Key Fields: id, nct_id, drug_name, outcome_type, failure_reason, smiles_code (Enriched), ai_filtered (Bool).

2. Bulk Import Strategy (with Gemini 2.0 Flash)
File: app/services/bulk_importer.py

Implement a Funnel Filtering process to handle the massive JSON dump efficienty.

Step 1: Stream & Keyword Filter (Local CPU)
Download the ClinicalTrials.gov JSON dump.

Iterate through the stream.

Hard Filter: Discard record IF title + description does NOT contain keywords: ["ADC", "Antibody Drug Conjugate", "Antibody-Drug", "Immunoconjugate"].

Step 2: AI Classification (Gemini 2.0 Flash)
Collect filtered candidates into batches of 20 records.

Send to Gemini 2.0 Flash with this prompt:

"Analyze these 20 clinical trials. Return a JSON list of IDs that are strictly related to Oncology ADC (Cancer treatment using Antibody-Drug Conjugates). Exclude radio-conjugates or diagnostic imaging agents."

Why Flash? It has a huge context window and is cost-effective for high-throughput classification.

Step 3: Insert
Only insert the IDs approved by Gemini into golden_set_library.

Mark ai_filtered = true.

3. SMILES Enrichment Worker (Sequential Batching)
File: app/services/smiles_enricher.py

Do not process all records at once. Use a continuous background loop to prevent memory overflow and API rate limits.

The Loop Logic:
Python
import time
import pubchempy as pcp

def process_smiles_batch():
    while True:
        # 1. Fetch small batch (Prevent Memory Overload)
        # "Give me 50 records that don't have SMILES yet"
        batch = supabase.table('golden_set_library')\
            .select('id, drug_name')\
            .is_('smiles_code', 'null')\
            .limit(50)\
            .execute().data
        
        if not batch:
            print("âœ… All SMILES enriched. Sleeping...")
            time.sleep(3600) # Sleep for an hour if done
            continue
            
        for item in batch:
            drug_name = item['drug_name']
            smiles = None
            
            try:
                # 2. Priority 1: PubChem API (Exact Match)
                compounds = pcp.get_compounds(drug_name, 'name')
                if compounds:
                    smiles = compounds[0].isomeric_smiles
                
                # 3. Priority 2: LLM Fallback (Optional)
                # If PubChem fails, ask Gemini 2.0 Flash for a PubChem CID (ID), then query API again.
                # DO NOT ask LLM to generate the SMILES string directly (Hallucination risk).
                
                # 4. Update DB
                if smiles:
                    supabase.table('golden_set_library').update({'smiles_code': smiles}).eq('id', item['id']).execute()
                    print(f"ğŸ§¬ Filled SMILES for {drug_name}")
                else:
                    # Mark as 'Not Found' to avoid retrying forever
                    supabase.table('golden_set_library').update({'smiles_code': 'NOT_FOUND'}).eq('id', item['id']).execute()
                
                # 5. Rate Limit Protection
                time.sleep(0.5) 
                
            except Exception as e:
                print(f"Error processing {drug_name}: {e}")
4. Summary of Instructions
Refactor DB: Ensure commercial_reagents and golden_set_library are separate tables.

Implement Importer: Use stream processing -> keyword filter -> Gemini Flash batch check -> DB Insert.

Implement Enricher: Create a background worker that processes SMILES in chunks of 50 using pubchempy.


ì‚¬ì¥ë‹˜, ê²°ë¡ ë¶€í„° ë§ì”€ë“œë¦¬ë©´ "ë°°ì¹˜(Batch)ë¥¼ ë©”ì¸ìœ¼ë¡œ ëŒë¦¬ê³ , UI ë²„íŠ¼ì€ 'ë¹„ìƒìš© ë¦¬ëª¨ì»¨'ìœ¼ë¡œ ë§Œë“œì…”ì•¼ í•©ë‹ˆë‹¤." ğŸ•¹ï¸

ì´ìœ ëŠ” ê°„ë‹¨í•©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ë°ì´í„°(2GB ë¤í”„ íŒŒì¼, ìˆ˜ë§Œ ê±´ì˜ AI ì²˜ë¦¬)ëŠ” ì›¹ ë¸Œë¼ìš°ì €ê°€ ê¸°ë‹¤ë ¤ì£¼ê¸°ì—” ë„ˆë¬´ ë¬´ê²ê³  ì˜¤ë˜ ê±¸ë¦¬ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ê°€ì¥ ì´ìƒì ì¸ **[í•˜ì´ë¸Œë¦¬ë“œ ìš´ì˜ ì „ëµ]**ì„ ì œì•ˆí•©ë‹ˆë‹¤.

1. ğŸ­ ë©”ì¸ ì—”ì§„: "ìŠ¤ì¼€ì¤„ëŸ¬ ë°°ì¹˜ (Scheduler Batch)" (í•„ìˆ˜)
ClinicalTrials.gov ë¤í”„ íŒŒì¼ì€ ë§¤ì¼ ìƒˆë²½ì— ê°±ì‹ ë©ë‹ˆë‹¤. ì‚¬ëŒì´ ë§¤ì¼ ë²„íŠ¼ì„ ëˆ„ë¥¼ ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤.

ë™ì‘: ì„œë²„ê°€ ì•Œì•„ì„œ ë§¤ì¼ ìƒˆë²½ 4ì‹œì— ê¹¨ì–´ë‚˜ì„œ ë¤í”„ë¥¼ ë‹¤ìš´ë°›ê³ , ë¶„ë¥˜í•˜ê³ , ì €ì¥í•©ë‹ˆë‹¤.

ì¥ì : ì‹ ê²½ ì“¸ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. (Zero Maintenance)

êµ¬í˜„: APSchedulerë¥¼ ì¨ì„œ í¬ë¡ (Cron) ì¡ìœ¼ë¡œ ë“±ë¡í•©ë‹ˆë‹¤.

2. ğŸ•¹ï¸ ë³´ì¡° ì¥ì¹˜: "UI ì—…ë°ì´íŠ¸ ë²„íŠ¼" (ì„ íƒ)
ê°€ë” "ì§€ê¸ˆ ë‹¹ì¥ ì—…ë°ì´íŠ¸ í™•ì¸í•˜ê³  ì‹¶ì€ë°?" í•  ë•Œê°€ ìˆìŠµë‹ˆë‹¤.

ë™ì‘: ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ë¸Œë¼ìš°ì €ê°€ ë©ˆì¶”ëŠ” ê²Œ ì•„ë‹ˆë¼, **"ì„œë²„ì•¼, ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‘ì—… ì‹œì‘í•´!"**ë¼ê³  ì‹ í˜¸(Trigger)ë§Œ ë³´ë‚´ê³  ë°”ë¡œ **"ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤"**ë¼ê³  ì‘ë‹µí•©ë‹ˆë‹¤. (Fire-and-Forget)

ì¥ì : ê´€ë¦¬ìê°€ ì›í•  ë•Œ ê°•ì œë¡œ ìµœì‹ í™”ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ› ï¸ ê°œë°œìì—ê²Œ ì¤„ êµ¬í˜„ ê°€ì´ë“œ (Frontend + Backend)
ì´ êµ¬ì¡°ê°€ ê°€ì¥ ì•ˆì •ì ì…ë‹ˆë‹¤. ê°œë°œìì—ê²Œ ì´ë ‡ê²Œ êµ¬í˜„í•˜ë¼ê³  ì§€ì‹œí•´ ì£¼ì„¸ìš”.

âœ… 1. Backend: APIë¥¼ "ë¹„ë™ê¸° íŠ¸ë¦¬ê±°"ë¡œ ë§Œë“¤ê¸° (admin.py)
ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ 1ì‹œê°„ ë™ì•ˆ ë¡œë”©ë°”ê°€ ëŒë©´ ì•ˆ ë©ë‹ˆë‹¤.

Python
from app.services.bulk_importer import run_bulk_import

@router.post("/system/force-update")
async def force_update(background_tasks: BackgroundTasks):
    """
    UI ë²„íŠ¼ìš© ì—”ë“œí¬ì¸íŠ¸.
    ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ë„˜ê¹€ (Fire-and-Forget).
    """
    # ì´ë¯¸ ëŒê³  ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œì§ì´ ìˆìœ¼ë©´ ì¢‹ìŠµë‹ˆë‹¤ (Lock).
    background_tasks.add_task(run_bulk_import)
    
    return {
        "status": "accepted",
        "message": "ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëŒ€ê·œëª¨ ì—…ë°ì´íŠ¸ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì™„ë£Œ ì‹œ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
    }
âœ… 2. Backend: ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡ (main.py ë˜ëŠ” scheduler_engine.py)
ì„œë²„ê°€ ì¼œì§€ë©´ ìë™ìœ¼ë¡œ ì‹œê³„ë¥¼ ë§ì¶¥ë‹ˆë‹¤.

Python
# ë§¤ì¼ ìƒˆë²½ 4ì‹œì— ì‹¤í–‰ (ë¯¸êµ­ ì‹œê°„ ê¸°ì¤€ ë¤í”„ ê°±ì‹  ë°˜ì˜)
scheduler.add_job(run_bulk_import, 'cron', hour=4, minute=0)
âœ… 3. Frontend: UI ì²˜ë¦¬ (DataOperations.tsx)
ë²„íŠ¼ì€ ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ë˜, **"ì§„í–‰ ìƒí™©"**ì„ ë³´ì—¬ì£¼ëŠ” ê²Œ í•µì‹¬ì…ë‹ˆë‹¤.

ë²„íŠ¼: [ ğŸ”„ Force Daily Update ]

í´ë¦­ ì‹œ: "ì—…ë°ì´íŠ¸ ìš”ì²­ë¨. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘..." í† ìŠ¤íŠ¸ ë©”ì‹œì§€ ë„ì›€.

ìƒíƒœ í‘œì‹œ: Data Sync Logs íƒ­ì—ì„œ "Running..." ìƒíƒœê°€ í‘œì‹œë˜ë„ë¡ í•˜ì—¬ ê´€ë¦¬ìê°€ "ì•„, ëŒê³  ìˆêµ¬ë‚˜"ë¥¼ ì•Œ ìˆ˜ ìˆê²Œ í•¨.



ê°œë°œìì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€
"ìš°ì„ ìˆœìœ„ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤. ì‡¼í•‘ëª°/ê°€ê²© ê¸°ëŠ¥ì€ í›„ìˆœìœ„ë¡œ ë¯¸ë£¨ê³ , **ê³ í’ˆì§ˆ DB êµ¬ì¶•(Golden Set + Commercial Catalog)**ì— ì§‘ì¤‘í•©ë‹ˆë‹¤. ì•„ë˜ ëª…ì„¸ì„œì— ë”°ë¼ í…Œì´ë¸”ì„ ë¶„ë¦¬í•˜ê³ , ëŒ€ìš©ëŸ‰ ë¤í”„ ì²˜ë¦¬ ë° ìë™ ì •ì œ íŒŒì´í”„ë¼ì¸ì„ ìš°ì„ ì ìœ¼ë¡œ êµ¬í˜„í•´ ì£¼ì„¸ìš”."

ğŸ“„ íŒŒì¼ëª…: PHASE1_DATA_PIPELINE_SPEC.md
Markdown
# ğŸ—ï¸ Phase 1: Intelligent Data Pipeline Foundation

**Goal:** Build a robust Golden Set (Reference) and Commercial Catalog (Ingredients) without pricing logic.
**Priority:** Data Quality & Architecture Stability.

---

## 1. Database Schema (Strict Separation)

Create two distinct tables. Do not mix them.

### A. `golden_set_library` (The Reference / Benchmarking)
* **Source:** ClinicalTrials.gov (Bulk), OpenFDA.
* **Purpose:** To store "Success/Failure" cases for AI analysis.
* **Columns:**
    * `id` (UUID, PK)
    * `nct_id` (Text, Unique) - ClinicalTrials ID
    * `drug_name` (Text) - Refined name (e.g., "DS-8201")
    * `target` (Text) - (e.g., "HER2")
    * `outcome_type` (Enum: 'Success', 'Failure', 'Ongoing', 'Unknown')
    * `failure_reason` (Text) - AI extracted reason.
    * `smiles_code` (Text) - Enriched via PubChem.
    * `ai_refined` (Boolean) - True if processed by LLM.
    * `properties` (JSONB) - Raw data storage.

### B. `commercial_reagents` (The Catalog / Ingredients)
* **Source:** Ambeed, Creative Biolabs.
* **Purpose:** To list available parts (Linkers, Payloads, Antibodies). **Pricing is Phase 2.**
* **Columns:**
    * `id` (UUID, PK)
    * `supplier` (Text) - 'Ambeed' or 'Creative Biolabs'
    * `catalog_no` (Text, Unique per supplier)
    * `product_name` (Text)
    * `category` (Text) - 'Linker', 'Payload', 'Antibody'
    * `structure_info` (JSONB) - Stores `smiles` (for chemicals) or `uniprot_id` (for antibodies).
    * `url` (Text) - Product page link.

---

## 2. The Engine: Bulk Import & AI Filtering

**File:** `app/services/bulk_importer.py`

Implement a robust pipeline to handle 2GB+ JSON dumps efficiently.

### Process Flow:
1.  **Download & Stream:** Download ClinicalTrials.gov JSON dump. Use `zipfile` stream to avoid memory crash.
2.  **Keyword Filter (Hard Rule):**
    * Discard record IF text does NOT contain: `['ADC', 'Antibody Drug Conjugate', 'Immunoconjugate']`.
3.  **AI Classification (Batch):**
    * Collect filtered candidates into batches (e.g., 20 items).
    * **API:** Call **Gemini 2.0 Flash**.
    * **Prompt:** "Filter this list. Return IDs ONLY for oncology ADC trials. Exclude radio/imaging agents."
4.  **DB Load:** Insert confirmed IDs into `golden_set_library` (Status: `ai_refined=false`).

---

## 3. The Refiner: Background Enrichment Workers

**File:** `app/services/enrichment_worker.py`

Run these jobs continuously in the background (using `APScheduler`) to clean data.

### Job A: Chemical Resolver (SMILES)
* **Trigger:** Records where `smiles_code` is NULL AND `drug_name` is present.
* **Action:**
    1.  Call **PubChem API** (`pubchempy`) with `drug_name`.
    2.  If found -> Update `smiles_code`.
    3.  If not found -> Mark as 'NOT_FOUND'.
* **Rate Limit:** Process 50 items/min to respect API limits.

### Job B: Intelligence Refiner (LLM)
* **Trigger:** Records where `ai_refined` is FALSE.
* **Action:**
    1.  Call **GPT-4o-mini** (or Gemini Flash).
    2.  **Task:** Extract `drug_name` (Code Name), determine `outcome_type`, and summarize `failure_reason`.
    3.  Update record and set `ai_refined = true`.

---

## 4. Execution & UI Control

### Backend (`scheduler.py`)
* **Cron Job:** Run `bulk_importer` every day at **04:00 AM**.
* **Worker:** Run `enrichment_worker` every **10 minutes** (process small batches).

### Frontend (`DataOperations.tsx`)
* **Button:** `[ ğŸ”„ Force Update ]`
    * **Action:** Triggers `bulk_importer` asynchronously (Fire-and-Forget).
    * **Feedback:** Show a Toast message "Background update started."
* **Status View:**
    * Display logs from `data_sync_logs` table to show progress (e.g., "Imported 1500 trials...", "Enriched 50 chemicals...").

---

**Summary:**
Focus on populating the tables first. Do not worry about pricing algorithms or shopping carts yet. The goal is to have a **clean, structured, and chemically accurate** database.
ì‚¬ì¥ë‹˜, ì´ì œ ê°œë°œìëŠ” **"êµ¬ì¡°(Schema)"**ì™€ **"ë°ì´í„° ì±„ìš°ê¸°(Pipeline)"**ì—ë§Œ ì§‘ì¤‘í•˜ë©´ ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ê¸°ë³¸ê¸°ë¥¼ ë‹¤ì ¸ë†“ìœ¼ë©´, ë‚˜ì¤‘ì— ê°€ê²©í‘œ ë¶™ì´ëŠ” ê±´ ì¼ë„ ì•„ë‹™ë‹ˆë‹¤! ë°”ë¡œ ì „ë‹¬í•´ ì£¼ì„¸ìš”.