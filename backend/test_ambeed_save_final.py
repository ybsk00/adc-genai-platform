import asyncio
import logging
import sys
import os

# backend ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.getcwd(), "backend"))

from app.services.ambeed_crawler import ambeed_crawler
from app.core.supabase import supabase

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("TestSave")

async def test_save():
    job_id = "test_run_001"
    limit = 2
    start_page = 10  # 1í˜ì´ì§€ëŠ” ì¤‘ë³µì´ ë§ìœ¼ë‹ˆ 10í˜ì´ì§€ë¶€í„° í…ŒìŠ¤íŠ¸
    
    print(f"\nğŸš€ í…ŒìŠ¤íŠ¸ ì‹œì‘: Ambeed ìˆ˜ì§‘ (Page {start_page}ë¶€í„° {limit}ê°œ)")
    
    # 1. ì‹¤í–‰ ì „ ê°œìˆ˜ í™•ì¸
    res_before = supabase.table("commercial_reagents").select("count", count="exact").execute()
    count_before = res_before.count
    print(f"ğŸ“Š ìˆ˜ì§‘ ì „ DB ë ˆì½”ë“œ ìˆ˜: {count_before}")
    
    # 2. í¬ë¡¤ëŸ¬ ì‹¤í–‰
    # (APIê°€ ì•„ë‹Œ ë‚´ë¶€ ë©”ì„œë“œë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ í™•ì¸)
    await ambeed_crawler.run(search_term="ADC Toxins", limit=limit, job_id=job_id, start_page=start_page)
    
    # 3. ì‹¤í–‰ í›„ ê°œìˆ˜ í™•ì¸
    res_after = supabase.table("commercial_reagents").select("count", count="exact").execute()
    count_after = res_after.count
    print(f"ğŸ“Š ìˆ˜ì§‘ í›„ DB ë ˆì½”ë“œ ìˆ˜: {count_after}")
    
    # 4. ìµœì‹  ì €ì¥ ë°ì´í„° 2ê°œ ì¶œë ¥
    latest = supabase.table("commercial_reagents")\
        .select("ambeed_cat_no, product_name, crawled_at")\
        .order("crawled_at", desc=True)\
        .limit(2)\
        .execute()
    
    print("\nâœ… ìµœê·¼ ì €ì¥ëœ ë°ì´í„° (DB ì§ì ‘ ì¡°íšŒ):")
    for item in latest.data:
        print(f"- {item['ambeed_cat_no']}: {item['product_name']} (ìˆ˜ì§‘ì‹œê°„: {item['crawled_at']})")

if __name__ == "__main__":
    asyncio.run(test_save())
