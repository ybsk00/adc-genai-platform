import os
import asyncio
import logging
import json
from typing import List, Dict, Any
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
env_path = os.path.join(os.path.dirname(__file__), '.env')
load_dotenv(env_path)

from app.core.config import settings
from app.core.supabase import supabase
from app.services.rag_service import rag_service

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Reindexer:
    def __init__(self):
        self.batch_size = 20
        self.delay_between_batches = 2.0  # API í• ë‹¹ëŸ‰ ê³ ë ¤ (TPM/RPM)

    async def reindex_golden_set(self):
        logger.info("ğŸš€ [Golden Set] ì¬ì¸ë±ì‹± ì‹œì‘...")
        try:
            # 1. ê¸°ì¡´ ì„ë² ë”© ì‚­ì œ (768ì°¨ì›ìœ¼ë¡œ ìƒˆë¡œ ì±„ìš°ê¸° ìœ„í•´)
            supabase.table("golden_set_embeddings").delete().neq("id", "00000000-0000-0000-0000-000000000000").execute()
            
            # 2. ì›ë³¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            response = supabase.table("golden_set_library").select("*").eq("status", "approved").execute()
            items = response.data
            logger.info(f"Found {len(items)} approved golden set items.")

            for i in range(0, len(items), self.batch_size):
                batch = items[i:i + self.batch_size]
                for item in batch:
                    props = item.get("properties", {}) or {}
                    await rag_service.index_golden_set_item(
                        item_id=item["id"],
                        description=item.get("description", ""),
                        properties=props
                    )
                logger.info(f"Indexed {min(i + self.batch_size, len(items))}/{len(items)} golden set items.")
                await asyncio.sleep(self.delay_between_batches)
            
            logger.info("âœ… [Golden Set] ì¬ì¸ë±ì‹± ì™„ë£Œ.")
        except Exception as e:
            logger.error(f"âŒ Golden Set ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {e}")

    async def reindex_commercial_reagents(self):
        logger.info("ğŸš€ [Commercial Reagents] ì¬ì¸ë±ì‹± ì‹œì‘...")
        try:
            response = supabase.table("commercial_reagents").select("id, product_name, target, properties, summary").execute()
            items = response.data
            logger.info(f"Found {len(items)} commercial reagents.")

            for i in range(0, len(items), self.batch_size):
                batch = items[i:i + self.batch_size]
                for item in batch:
                    # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ê²°í•©
                    content = f"Product: {item['product_name']}\nTarget: {item.get('target', 'N/A')}\n"
                    if item.get("summary"):
                        content += f"Summary: {item['summary']}\n"
                    props = item.get("properties", {}) or {}
                    for k, v in props.items():
                        if v: content += f"{k}: {v}\n"
                    
                    embedding = await rag_service.generate_embedding(content)
                    if embedding:
                        supabase.table("commercial_reagents").update({"embedding": embedding}).eq("id", item["id"]).execute()
                
                logger.info(f"Indexed {min(i + self.batch_size, len(items))}/{len(items)} commercial reagents.")
                await asyncio.sleep(self.delay_between_batches)
            
            logger.info("âœ… [Commercial Reagents] ì¬ì¸ë±ì‹± ì™„ë£Œ.")
        except Exception as e:
            logger.error(f"âŒ Commercial Reagents ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {e}")

    async def reindex_knowledge_base(self):
        logger.info("ğŸš€ [Knowledge Base] ì¬ì¸ë±ì‹± ì‹œì‘...")
        try:
            # knowledge_base í…Œì´ë¸”ì€ content(ì´ˆë¡)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ë² ë”©
            response = supabase.table("knowledge_base").select("id, title, content, abstract").execute()
            items = response.data
            logger.info(f"Found {len(items)} knowledge base items.")

            for i in range(0, len(items), self.batch_size):
                batch = items[i:i + self.batch_size]
                for item in batch:
                    # Use abstract if available, otherwise content
                    text_content = item.get('abstract') or item.get('content') or ""
                    content = f"Title: {item['title']}\nContent: {text_content}"
                    embedding = await rag_service.generate_embedding(content)
                    if embedding:
                        # knowledge_base í…Œì´ë¸”ì— embedding ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸ í•„ìš” (ì—†ìœ¼ë©´ ì¶”ê°€ ë¡œì§ í•„ìš”í•  ìˆ˜ ìˆìŒ)
                        try:
                            supabase.table("knowledge_base").update({"embedding": embedding}).eq("id", item["id"]).execute()
                        except Exception as inner_e:
                            logger.warning(f"Skipping KB item {item['id']} (Check if embedding column exists): {inner_e}")
                
                logger.info(f"Indexed {min(i + self.batch_size, len(items))}/{len(items)} knowledge base items.")
                await asyncio.sleep(self.delay_between_batches)
            
            logger.info("âœ… [Knowledge Base] ì¬ì¸ë±ì‹± ì™„ë£Œ.")
        except Exception as e:
            logger.error(f"âŒ Knowledge Base ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {e}")

    async def run_all(self):
        await self.reindex_golden_set()
        await self.reindex_commercial_reagents()
        await self.reindex_knowledge_base()

if __name__ == "__main__":
    reindexer = Reindexer()
    asyncio.run(reindexer.run_all())
