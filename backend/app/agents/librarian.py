"""
The Librarian Agent - Dynamic Substructure RAG
ìŠ¤ìºí´ë“œ ì¶”ì¶œ ë° ì§€ì‹ë² ì´ìŠ¤ ì˜ë¯¸ë¡ ì  ë§¤í•‘

Phase 1 Enhancement:
- Gemini 3.0 Pro í†µí•© (evidence synthesis)
- Real-time Streaming UI ì—°ë™
- Target Normalization í™œìš©
"""
from typing import List, Dict, Any, Optional
import logging
import json
from datetime import datetime

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage

from app.agents.base_agent import BaseDesignAgent, AgentOutput
from app.agents.design_state import DesignSessionState
from app.services.rag_service import RAGService
from app.core.supabase import get_supabase_client
from app.core.config import settings
from app.core.websocket_hub import websocket_hub

logger = logging.getLogger(__name__)


class LibrarianAgent(BaseDesignAgent):
    """
    The Librarian: Dynamic Substructure RAG ì—ì´ì „íŠ¸

    í•µì‹¬ ê¸°ëŠ¥:
    1. ì…ë ¥ SMILESì—ì„œ ìŠ¤ìºí´ë“œ/ì„œë¸ŒìŠ¤íŠ¸ëŸ­ì²˜ ì¶”ì¶œ (Murcko decomposition)
    2. ìŠ¤ìºí´ë“œë¥¼ ë²¡í„° ì„ë² ë”©í•˜ì—¬ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ì„± ê²€ìƒ‰
    3. knowledge_base + golden_setì—ì„œ ê´€ë ¨ ë¬¸í—Œ/êµ¬ì¡° ê²€ìƒ‰
    4. ê´€ë ¨ PMID ë° ê·¼ê±° ìë£Œ ì œê³µ
    5. [Phase 1] Geminië¥¼ í™œìš©í•œ ê·¼ê±° í•©ì„± ë° ì„¤ëª… ìƒì„±

    Phase 1 Enhancement:
    - Gemini Proë¥¼ ì‚¬ìš©í•œ evidence synthesis
    - Target Normalizationì„ í†µí•œ ì •í™•í•œ ê²€ìƒ‰
    - Real-time Streamingì„ í†µí•œ ì§„í–‰ ìƒí™© ì „ì†¡
    """

    name = "librarian"

    def __init__(self):
        super().__init__()
        self.rag_service = RAGService()
        # Gemini ëª¨ë¸ ì´ˆê¸°í™” (Phase 1)
        self.llm = ChatGoogleGenerativeAI(
            model=settings.GEMINI_PRO_MODEL_ID,  # gemini-2.0-flash
            google_api_key=settings.GOOGLE_API_KEY,
            temperature=0.2  # ë‚®ì€ temperatureë¡œ ì¼ê´€ëœ ê·¼ê±° ìƒì„±
        )

    async def execute(self, state: DesignSessionState) -> AgentOutput:
        """ë©”ì¸ ì‹¤í–‰"""
        session_id = state["session_id"]
        smiles = state["current_smiles"]
        target_antigen = state.get("target_antigen", "")

        await self._log_start(session_id, {
            "smiles_length": len(smiles) if smiles else 0,
            "target": target_antigen
        })

        try:
            # [Streaming] ì‹œì‘ ë¡œê·¸
            await websocket_hub.stream_agent_log(
                session_id, "info",
                f"ë¬¸í—Œ ê²€ìƒ‰ ì‹œì‘: íƒ€ê²Ÿ '{target_antigen}'",
                emoji="ğŸ“š", agent_name="librarian"
            )

            # 1. ìŠ¤ìºí´ë“œ ì¶”ì¶œ
            await websocket_hub.stream_progress(session_id, 10, "ìŠ¤ìºí´ë“œ ì¶”ì¶œ ì¤‘")
            scaffolds = self._extract_scaffolds(smiles)
            await websocket_hub.stream_agent_log(
                session_id, "success",
                f"{len(scaffolds)}ê°œ ìŠ¤ìºí´ë“œ ì¶”ì¶œ ì™„ë£Œ",
                emoji="âœ…", agent_name="librarian"
            )

            # 2. íƒ€ê²Ÿ ì •ê·œí™” í™œìš© (Phase 1)
            normalized_target = await self._normalize_target(target_antigen)
            if normalized_target != target_antigen:
                await websocket_hub.stream_agent_log(
                    session_id, "info",
                    f"íƒ€ê²Ÿ ì •ê·œí™”: '{target_antigen}' â†’ '{normalized_target}'",
                    emoji="ğŸ”„", agent_name="librarian"
                )

            # 3. ê° ìŠ¤ìºí´ë“œì— ëŒ€í•´ ì§€ì‹ë² ì´ìŠ¤ ë§¤í•‘
            await websocket_hub.stream_progress(session_id, 30, "ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘")
            knowledge_results = []
            for idx, scaffold in enumerate(scaffolds):
                # 3.1 ê¸°ì¡´ ë§¤í•‘ ê²€ìƒ‰ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
                mapping = await self._get_or_create_scaffold_mapping(scaffold)

                # 3.2 ì˜ë¯¸ë¡ ì  ê²€ìƒ‰
                semantic_hits = await self._semantic_search(
                    scaffold["smiles"],
                    normalized_target,
                    top_k=5
                )

                # 3.3 Golden Set ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
                golden_set_hits = await self._search_golden_set_by_similarity(
                    normalized_target,
                    threshold=0.6
                )

                knowledge_results.append({
                    "scaffold_type": scaffold["type"],
                    "scaffold_smiles": scaffold["smiles"],
                    "semantic_hits": semantic_hits,
                    "golden_set_hits": golden_set_hits,
                    "mapping_id": str(mapping["id"]) if mapping else None
                })

                await websocket_hub.stream_progress(
                    session_id,
                    30 + (idx + 1) / len(scaffolds) * 30,
                    f"ìŠ¤ìºí´ë“œ {idx + 1}/{len(scaffolds)} ê²€ìƒ‰ ì™„ë£Œ"
                )

            # 4. í†µí•© ê·¼ê±° ìƒì„±
            await websocket_hub.stream_progress(session_id, 70, "ê·¼ê±° ìë£Œ í†µí•© ì¤‘")
            evidence = self._compile_evidence(knowledge_results)

            # 5. [Phase 1] Geminië¥¼ ì‚¬ìš©í•œ ê·¼ê±° í•©ì„±
            await websocket_hub.stream_progress(session_id, 80, "AI ê·¼ê±° í•©ì„± ì¤‘")
            await websocket_hub.stream_agent_log(
                session_id, "info",
                "Gemini Proë¡œ ê·¼ê±° í•©ì„± ì¤‘...",
                emoji="ğŸ¤–", agent_name="librarian"
            )
            synthesized_evidence = await self._synthesize_evidence_with_gemini(
                target_antigen=normalized_target,
                scaffolds=scaffolds,
                semantic_hits=[h for kr in knowledge_results for h in kr.get("semantic_hits", [])],
                golden_set_hits=[h for kr in knowledge_results for h in kr.get("golden_set_hits", [])]
            )

            await websocket_hub.stream_progress(session_id, 100, "ê²€ìƒ‰ ì™„ë£Œ")
            await websocket_hub.stream_agent_log(
                session_id, "success",
                f"ê²€ìƒ‰ ì™„ë£Œ: {len(evidence['pmids'])} ë¬¸í—Œ, {len(evidence['golden_sets'])} Golden Set ì°¸ì¡°",
                emoji="âœ…", agent_name="librarian"
            )

            # [Streaming] ì°¸ì¡° ë¬¸í—Œ ì „ì†¡
            await websocket_hub.broadcast_librarian_references(
                session_id,
                references=[{
                    "id": h.get("id"),
                    "pmid": h.get("pmid"),
                    "title": h.get("title", ""),
                    "relevance_score": h.get("relevance_score", 0),
                    "summary": h.get("summary", "")
                } for kr in knowledge_results for h in kr.get("semantic_hits", [])],
                golden_set_refs=evidence["golden_sets"]
            )

            output = AgentOutput(
                success=True,
                data={
                    "scaffolds": scaffolds,
                    "knowledge_results": knowledge_results,
                    "evidence_summary": synthesized_evidence.get("summary", evidence["summary"]),
                    "evidence_reasoning": synthesized_evidence.get("reasoning", ""),
                    "pmid_references": evidence["pmids"],
                    "golden_set_references": evidence["golden_sets"],
                    "normalized_target": normalized_target
                },
                reasoning=synthesized_evidence.get("reasoning",
                    f"Extracted {len(scaffolds)} scaffolds, found {len(evidence['pmids'])} literature references"),
                confidence_score=0.85 if evidence["pmids"] else 0.6,
                referenced_knowledge_ids=evidence["knowledge_ids"],
                referenced_pmids=evidence["pmids"]
            )

            await self._log_complete(session_id, output)
            return output

        except Exception as e:
            logger.exception(f"[librarian] Error: {e}")
            await websocket_hub.stream_agent_log(
                session_id, "error",
                f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}",
                emoji="âŒ", agent_name="librarian"
            )
            await self._log_error(session_id, str(e))
            return AgentOutput(
                success=False,
                data={},
                error=str(e)
            )

    def _extract_scaffolds(self, smiles: str) -> List[Dict[str, Any]]:
        """
        ìŠ¤ìºí´ë“œ ì¶”ì¶œ (RDKit Murcko Decomposition)

        Returns: [{type, smiles}, ...]
        """
        if not smiles:
            return []

        scaffolds = []

        try:
            from rdkit import Chem
            from rdkit.Chem.Scaffolds import MurckoScaffold

            mol = Chem.MolFromSmiles(smiles)
            if not mol:
                logger.warning(f"[librarian] Invalid SMILES: {smiles[:50]}")
                return []

            # 1. Generic Scaffold
            try:
                generic = MurckoScaffold.MakeScaffoldGeneric(
                    MurckoScaffold.GetScaffoldForMol(mol)
                )
                if generic:
                    generic_smiles = Chem.MolToSmiles(generic)
                    scaffolds.append({
                        "type": "murcko_generic",
                        "smiles": generic_smiles
                    })
            except Exception as e:
                logger.warning(f"[librarian] Generic scaffold error: {e}")

            # 2. Framework
            try:
                framework = MurckoScaffold.GetScaffoldForMol(mol)
                if framework:
                    framework_smiles = Chem.MolToSmiles(framework)
                    scaffolds.append({
                        "type": "murcko_framework",
                        "smiles": framework_smiles
                    })
            except Exception as e:
                logger.warning(f"[librarian] Framework error: {e}")

            # 3. Ring Systems (ê°„ë‹¨í•œ ì¶”ì¶œ)
            try:
                ring_info = mol.GetRingInfo()
                if ring_info.NumRings() > 0:
                    scaffolds.append({
                        "type": "ring_system",
                        "smiles": smiles,  # ì›ë³¸ ìœ ì§€
                        "ring_count": ring_info.NumRings()
                    })
            except Exception as e:
                logger.warning(f"[librarian] Ring system error: {e}")

        except ImportError:
            logger.warning("[librarian] RDKit not available, skipping scaffold extraction")
            # Fallback: ì›ë³¸ SMILES ë°˜í™˜
            scaffolds.append({
                "type": "original",
                "smiles": smiles
            })

        except Exception as e:
            logger.error(f"[librarian] Scaffold extraction error: {e}")

        return scaffolds

    async def _get_or_create_scaffold_mapping(self, scaffold: Dict) -> Optional[Dict]:
        """ìŠ¤ìºí´ë“œ ë§¤í•‘ ì¡°íšŒ ë˜ëŠ” ì‹ ê·œ ìƒì„±"""
        try:
            # ê¸°ì¡´ ë§¤í•‘ ê²€ìƒ‰
            result = self.supabase.table("scaffold_knowledge_mapping").select("*").eq(
                "scaffold_smiles", scaffold["smiles"]
            ).eq(
                "scaffold_type", scaffold["type"]
            ).single().execute()

            if result.data:
                # ì¡°íšŒ ì¹´ìš´íŠ¸ ì¦ê°€
                self.supabase.table("scaffold_knowledge_mapping").update({
                    "retrieval_count": result.data["retrieval_count"] + 1,
                    "last_retrieved_at": datetime.utcnow().isoformat()
                }).eq("id", result.data["id"]).execute()
                return result.data

            # ì‹ ê·œ ë§¤í•‘ ìƒì„±
            new_mapping = self.supabase.table("scaffold_knowledge_mapping").insert({
                "scaffold_smiles": scaffold["smiles"],
                "scaffold_type": scaffold["type"],
                "scaffold_description": f"Scaffold of type {scaffold['type']}",
                "retrieval_count": 1,
                "last_retrieved_at": datetime.utcnow().isoformat()
            }).execute()

            return new_mapping.data[0] if new_mapping.data else None

        except Exception as e:
            logger.warning(f"[librarian] Scaffold mapping error: {e}")
            return None

    async def _semantic_search(
        self,
        scaffold_smiles: str,
        target: str,
        top_k: int = 5
    ) -> List[Dict]:
        """ì˜ë¯¸ë¡ ì  ê²€ìƒ‰: ìŠ¤ìºí´ë“œ + íƒ€ê²Ÿ ê¸°ë°˜ knowledge_base ê²€ìƒ‰"""
        query = f"ADC scaffold structure {scaffold_smiles[:50]} targeting {target}"

        try:
            # RAG ê²€ìƒ‰
            results = await self.rag_service.search(
                query=query,
                top_k=top_k
            )

            return [{
                "id": r.get("id"),
                "title": r.get("title", ""),
                "relevance_score": r.get("score", 0),
                "pmid": r.get("pmid") or r.get("metadata", {}).get("pmid"),
                "summary": r.get("summary", "")[:200]
            } for r in results]

        except Exception as e:
            logger.warning(f"[librarian] Semantic search error: {e}")
            return []

    async def _search_golden_set_by_similarity(
        self,
        target: str,
        threshold: float = 0.6
    ) -> List[Dict]:
        """Golden Setì—ì„œ íƒ€ê²Ÿ ê¸°ë°˜ ê²€ìƒ‰"""
        try:
            result = self.supabase.table("golden_set").select(
                "id, drug_name, target_1, target_2, indication, clinical_status"
            ).or_(
                f"target_1.ilike.%{target}%,target_2.ilike.%{target}%"
            ).limit(10).execute()

            return [{
                "id": str(g["id"]),
                "name": g.get("drug_name"),
                "target": g.get("target_1"),
                "status": g.get("clinical_status")
            } for g in (result.data or [])]

        except Exception as e:
            logger.warning(f"[librarian] Golden Set search error: {e}")
            return []

    def _compile_evidence(self, knowledge_results: List[Dict]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ê·¼ê±° ìë£Œ ìƒì„±"""
        pmids = set()
        knowledge_ids = []
        golden_sets = []
        summaries = []

        for result in knowledge_results:
            for hit in result.get("semantic_hits", []):
                if hit.get("pmid"):
                    pmids.add(hit["pmid"])
                if hit.get("id"):
                    knowledge_ids.append(hit["id"])

            for gs in result.get("golden_set_hits", []):
                golden_sets.append({
                    "id": gs["id"],
                    "name": gs.get("name"),
                    "target": gs.get("target")
                })

            summaries.append(
                f"{result['scaffold_type']}: {len(result.get('semantic_hits', []))} refs"
            )

        return {
            "pmids": list(pmids),
            "knowledge_ids": knowledge_ids,
            "golden_sets": golden_sets[:5],
            "summary": "; ".join(summaries) if summaries else "No references found"
        }

    async def link_scaffold_to_knowledge(
        self,
        scaffold_mapping_id: str,
        knowledge_ids: List[int],
        golden_set_ids: List[str],
        pmids: List[str]
    ):
        """ìŠ¤ìºí´ë“œ ë§¤í•‘ì— ì§€ì‹ë² ì´ìŠ¤ ë§í¬ ì¶”ê°€"""
        try:
            self.supabase.table("scaffold_knowledge_mapping").update({
                "linked_knowledge_ids": knowledge_ids,
                "linked_golden_set_ids": golden_set_ids,
                "linked_pmids": pmids,
                "updated_at": datetime.utcnow().isoformat()
            }).eq("id", scaffold_mapping_id).execute()
        except Exception as e:
            logger.error(f"[librarian] Failed to link scaffold: {e}")

    # ============== Phase 1: Gemini Integration ==============

    async def _normalize_target(self, target: str) -> str:
        """
        [Phase 1] Target Normalization
        target_synonyms í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ì—¬ íƒ€ê²Ÿëª…ì„ ì •ê·œí™”í•©ë‹ˆë‹¤.
        """
        if not target:
            return target

        try:
            # RPC í•¨ìˆ˜ í˜¸ì¶œ (normalize_target)
            result = self.supabase.rpc("normalize_target", {"input_target": target}).execute()
            if result.data:
                return result.data
            return target
        except Exception as e:
            logger.warning(f"[librarian] Target normalization failed: {e}")
            return target

    async def _synthesize_evidence_with_gemini(
        self,
        target_antigen: str,
        scaffolds: List[Dict],
        semantic_hits: List[Dict],
        golden_set_hits: List[Dict]
    ) -> Dict[str, Any]:
        """
        [Phase 1] Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì§‘ëœ ê·¼ê±°ë¥¼ í•©ì„±í•©ë‹ˆë‹¤.

        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ê·¼ê±° ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        if not (semantic_hits or golden_set_hits):
            return {
                "summary": "ê´€ë ¨ ë¬¸í—Œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                "reasoning": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê·¼ê±° í•©ì„±ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }

        system_prompt = """ë‹¹ì‹ ì€ ADC(Antibody-Drug Conjugate) ê°œë°œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ë¬¸í—Œê³¼ Golden Set ì°¸ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì„¤ê³„ ê·¼ê±°ë¥¼ í•©ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ì‘ë‹µ í˜•ì‹ (JSON):
{
    "summary": "í•µì‹¬ ê·¼ê±° ìš”ì•½ (2-3ë¬¸ì¥)",
    "reasoning": "ìƒì„¸ ë¶„ì„ ë‚´ìš©",
    "key_findings": ["ì£¼ìš” ë°œê²¬ì‚¬í•­ ë¦¬ìŠ¤íŠ¸"],
    "recommendations": ["ì„¤ê³„ ê¶Œì¥ì‚¬í•­ ë¦¬ìŠ¤íŠ¸"],
    "confidence": 0.0~1.0 (ê·¼ê±° ì‹ ë¢°ë„)
}"""

        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        literature_summary = "\n".join([
            f"- [{h.get('pmid', 'N/A')}] {h.get('title', 'Unknown')}: {h.get('summary', '')[:100]}..."
            for h in semantic_hits[:5]
        ]) if semantic_hits else "ê²€ìƒ‰ëœ ë¬¸í—Œ ì—†ìŒ"

        golden_set_summary = "\n".join([
            f"- {h.get('name', 'Unknown')}: Target={h.get('target', 'N/A')}, Status={h.get('status', 'N/A')}"
            for h in golden_set_hits[:5]
        ]) if golden_set_hits else "ê²€ìƒ‰ëœ Golden Set ì—†ìŒ"

        scaffold_summary = "\n".join([
            f"- {s.get('type', 'unknown')}: {s.get('smiles', '')[:50]}..."
            for s in scaffolds[:3]
        ]) if scaffolds else "ì¶”ì¶œëœ ìŠ¤ìºí´ë“œ ì—†ìŒ"

        user_prompt = f"""íƒ€ê²Ÿ í•­ì›: {target_antigen}

ì¶”ì¶œëœ ìŠ¤ìºí´ë“œ:
{scaffold_summary}

ê´€ë ¨ ë¬¸í—Œ:
{literature_summary}

Golden Set ì°¸ì¡° (FDA ìŠ¹ì¸ ADC):
{golden_set_summary}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ADC ì„¤ê³„ ê·¼ê±°ë¥¼ í•©ì„±í•´ì£¼ì„¸ìš”."""

        try:
            response = await self.llm.ainvoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ])

            content = response.content
            # JSON íŒŒì‹± ì‹œë„
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]

            data = json.loads(content)
            return {
                "summary": data.get("summary", ""),
                "reasoning": data.get("reasoning", ""),
                "key_findings": data.get("key_findings", []),
                "recommendations": data.get("recommendations", []),
                "confidence": data.get("confidence", 0.7)
            }

        except json.JSONDecodeError as e:
            logger.warning(f"[librarian] Gemini JSON parse error: {e}")
            # Fallback: ì›ë³¸ ì‘ë‹µ ì‚¬ìš©
            return {
                "summary": response.content[:500] if response else "",
                "reasoning": "Gemini ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"
            }

        except Exception as e:
            logger.error(f"[librarian] Gemini synthesis error: {e}")
            return {
                "summary": f"ë¬¸í—Œ {len(semantic_hits)}ê°œ, Golden Set {len(golden_set_hits)}ê°œ ê²€ìƒ‰ë¨",
                "reasoning": f"AI í•©ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }

    async def _search_antibody_library(
        self,
        target: str,
        top_k: int = 5
    ) -> List[Dict]:
        """
        [Phase 1] antibody_libraryì—ì„œ íƒ€ê²Ÿ ê¸°ë°˜ í•­ì²´ ê²€ìƒ‰

        target_normalized ì»¬ëŸ¼ì„ í™œìš©í•œ ê²€ìƒ‰
        """
        try:
            result = self.supabase.table("antibody_library").select(
                "id, product_name, cat_no, related_disease, target_normalized"
            ).or_(
                f"target_normalized.ilike.%{target}%,related_disease.ilike.%{target}%"
            ).limit(top_k).execute()

            return result.data or []

        except Exception as e:
            logger.warning(f"[librarian] Antibody library search error: {e}")
            return []

    # ============== One-Click Navigator Support ==============

    async def find_antibodies_by_disease(
        self,
        disease_name: str,
        top_k: int = 3
    ) -> List[Dict[str, Any]]:
        """
        [One-Click Navigator] ì§ˆí™˜ëª… ê¸°ë°˜ ìµœì  í•­ì²´ ê²€ìƒ‰

        Args:
            disease_name: ì§ˆí™˜ëª… (ì˜ˆ: "Pancreatic Cancer")
            top_k: ë°˜í™˜í•  í•­ì²´ ìˆ˜

        Returns:
            List[Dict]: í•­ì²´ í›„ë³´ ëª©ë¡
        """
        try:
            # 1. ì§ˆí™˜ëª… ì„ë² ë”© ìƒì„±
            disease_embedding = await self.rag_service.generate_embedding(
                f"Disease: {disease_name}, treatment target proteins, therapeutic antibodies"
            )

            # 2. antibody_library ë²¡í„° ê²€ìƒ‰
            results = self.supabase.rpc("match_antibody_by_disease", {
                "query_embedding": disease_embedding,
                "match_threshold": 0.5,
                "match_count": top_k * 2
            }).execute()

            antibodies = results.data or []

            # 3. ì„ìƒ ë°ì´í„° ê¸°ë°˜ ì¬ìˆœìœ„í™”
            scored_antibodies = []
            for ab in antibodies:
                clinical_score = self._calculate_antibody_clinical_score(ab)
                scored_antibodies.append({
                    **ab,
                    "clinical_score": clinical_score,
                    "combined_score": ab.get("similarity", 0.5) * 0.4 + clinical_score * 0.6
                })

            # 4. Top K ì„ ì •
            top_antibodies = sorted(
                scored_antibodies,
                key=lambda x: x["combined_score"],
                reverse=True
            )[:top_k]

            # 5. Fallback: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì—†ìœ¼ë©´ ì§ì ‘ ê²€ìƒ‰
            if not top_antibodies:
                logger.info(f"[librarian] Vector search empty, trying direct search: {disease_name}")
                return await self._direct_disease_antibody_search(disease_name, top_k)

            return top_antibodies

        except Exception as e:
            logger.error(f"[librarian] find_antibodies_by_disease error: {e}")
            return await self._direct_disease_antibody_search(disease_name, top_k)

    async def _direct_disease_antibody_search(
        self,
        disease_name: str,
        top_k: int
    ) -> List[Dict[str, Any]]:
        """ì§ì ‘ í‚¤ì›Œë“œ ê²€ìƒ‰ (fallback)"""
        try:
            result = self.supabase.table("antibody_library").select(
                "id, product_name, target_normalized, isotype, related_disease, full_spec"
            ).ilike(
                "related_disease", f"%{disease_name}%"
            ).limit(top_k).execute()

            if result.data:
                return [
                    {
                        "id": ab["id"],
                        "name": ab["product_name"],
                        "target_protein": ab.get("target_normalized", "Unknown"),
                        "isotype": ab.get("isotype"),
                        "related_disease": ab.get("related_disease"),
                        "full_spec": ab.get("full_spec"),
                        "clinical_score": 0.5,
                        "combined_score": 0.5,
                        "similarity": 0.5
                    }
                    for ab in result.data
                ]

            # Golden Setì—ì„œ ê²€ìƒ‰
            gs_result = self.supabase.table("golden_set").select(
                "id, drug_name, target_1, indication, orr_pct, os_months"
            ).ilike(
                "indication", f"%{disease_name}%"
            ).limit(top_k).execute()

            return [
                {
                    "id": gs["id"],
                    "name": gs["drug_name"],
                    "target_protein": gs.get("target_1", "Unknown"),
                    "isotype": None,
                    "related_disease": gs.get("indication"),
                    "full_spec": None,
                    "orr_pct": gs.get("orr_pct"),
                    "os_months": gs.get("os_months"),
                    "clinical_score": 0.7,
                    "combined_score": 0.7,
                    "similarity": 0.7
                }
                for gs in (gs_result.data or [])
            ]

        except Exception as e:
            logger.error(f"[librarian] Direct disease search error: {e}")
            return []

    def _calculate_antibody_clinical_score(self, ab: Dict[str, Any]) -> float:
        """í•­ì²´ì˜ ì„ìƒ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0

        # ORR (40%)
        orr = ab.get("orr_pct", 0) or 0
        score += (orr / 100) * 0.4

        # OS (30%)
        os_months = ab.get("os_months", 0) or 0
        score += min(os_months / 36, 1.0) * 0.3

        # ê¸°ë³¸ ì ìˆ˜ (30%)
        score += 0.3

        return min(score, 1.0)
