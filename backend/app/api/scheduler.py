"""
Data Worker API - ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ ë™ê¸°í™” (AstraForge 2.0)
DB ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬(Supabase) ë° PubChem í™”í•™ ì •ë³´ ì—°ë™
"""
from fastapi import APIRouter, BackgroundTasks, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta, timezone
from uuid import uuid4
import aiohttp
import asyncio
from Bio import Entrez
import logging
import json
import subprocess
import sys
import os

from app.core.config import settings
from app.core.supabase import supabase
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from app.services.openfda_service import openfda_service
from app.services.scheduler_engine import scheduler_engine
from app.services.chemical_resolver import chemical_resolver
from app.services.job_lock import job_lock

router = APIRouter()
logger = logging.getLogger(__name__)

# --- Job Manager (In-Memory Tracking) ---
class JobManager:
    def __init__(self):
        self.active_jobs = set()

    def add_job(self, job_id: str):
        self.active_jobs.add(job_id)

    def remove_job(self, job_id: str):
        if job_id in self.active_jobs:
            self.active_jobs.remove(job_id)

    def is_active(self, job_id: str) -> bool:
        return job_id in self.active_jobs

job_manager = JobManager()

# Entrez ì„¤ì •
Entrez.email = settings.NCBI_EMAIL
if settings.NCBI_API_KEY:
    Entrez.api_key = settings.NCBI_API_KEY

class DataSourceSetting(BaseModel):
    source_id: str
    auto_sync: bool
    sync_interval_hours: int

class SyncJobResponse(BaseModel):
    job_id: str
    status: str
    message: str

class SyncJobStatus(BaseModel):
    id: str
    status: str
    source: str
    records_found: int
    records_drafted: int
    last_processed_page: Optional[int] = 0
    started_at: str
    completed_at: Optional[str] = None
    errors: List[str] = []

# --- DB ê¸°ë°˜ ìƒíƒœ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹° ---

async def update_job_status(job_id: str, **kwargs):
    """DBì˜ sync_jobs í…Œì´ë¸” ìƒíƒœ ì—…ë°ì´íŠ¸"""
    try:
        # errors í•„ë“œê°€ ìˆìœ¼ë©´ JSON ì§ë ¬í™” í™•ì¸
        if "errors" in kwargs and isinstance(kwargs["errors"], list):
            kwargs["errors"] = kwargs["errors"] # Supabase client handles list to jsonb
        
        # message í•„ë“œëŠ” sync_jobs í…Œì´ë¸”ì— ì—†ìœ¼ë¯€ë¡œ ì œê±°
        if "message" in kwargs:
            del kwargs["message"]

        # DB ì—…ë°ì´íŠ¸ ì‹¤í–‰
        try:
            supabase.table("sync_jobs").update(kwargs).eq("id", job_id).execute()
        except Exception as e:
            # last_processed_page ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ìë™ í´ë°±
            if "last_processed_page" in str(e) or "42703" in str(e):
                logger.warning("âš ï¸ 'last_processed_page' column missing. Retrying without it.")
                if "last_processed_page" in kwargs:
                    del kwargs["last_processed_page"]
                supabase.table("sync_jobs").update(kwargs).eq("id", job_id).execute()
            else:
                raise e
    except Exception as e:
        logger.error(f"Failed to update job status in DB: {e}")

async def get_job_from_db(job_id: str) -> Optional[dict]:
    """DBì—ì„œ ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    try:
        res = supabase.table("sync_jobs").select("*").eq("id", job_id).execute()
        return res.data[0] if res.data else None
    except Exception as e:
        logger.error(f"Failed to get job from DB: {e}")
        return None

async def is_cancelled(job_id: str) -> bool:
    """ì¤‘ë‹¨ ìš”ì²­ ì—¬ë¶€ í™•ì¸"""
    job = await get_job_from_db(job_id)
    return job.get("cancel_requested", False) if job else False

async def run_isolated_crawler(crawler_type: str, category: str, limit: int, job_id: str, start_page: int = 1):
    """ì‹¤í–‰ ê²©ë¦¬ë¥¼ ìœ„í•´ ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    log_file = f"crawler_debug_{job_id}.log"
    try:
        # ì ˆëŒ€ ê²½ë¡œ ê³„ì‚°
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        script_path = os.path.join(base_dir, "run_crawler.py")
        
        if not os.path.exists(script_path):
            logger.error(f"âŒ Crawler script not found at: {script_path}")
            await update_job_status(job_id, status="failed", errors=[f"Script not found: {script_path}"])
            return

        cmd = [
            sys.executable,
            script_path,
            "--crawler", crawler_type,
            "--category", category,
            "--limit", str(limit),
            "--job_id", job_id,
            "--start_page", str(start_page)
        ]
        
        logger.info(f"ğŸš€ Launching isolated crawler (Absolute Path): {' '.join(cmd)}")
        
        # ë¡œê·¸ íŒŒì¼ ìƒì„±
        with open(log_file, "w") as f:
            f.write(f"Started at: {datetime.now()}\nCommand: {' '.join(cmd)}\n\n")

        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            env={**os.environ, "GRPC_ENABLE_FORK_SUPPORT": "false", "PYTHONPATH": base_dir}
        )
        
        # ë¹„ë™ê¸°ë¡œ ë¡œê·¸ ìº¡ì²˜ (ì„œë²„ ë¡œê·¸ì— ì¶œë ¥)
        stdout, stderr = await process.communicate()
        
        if stdout: logger.info(f"ğŸ“ Crawler Output: {stdout.decode()[:500]}")
        if stderr: logger.error(f"âš ï¸ Crawler Error Output: {stderr.decode()[:500]}")

        if process.returncode != 0:
            error_msg = stderr.decode().strip() if stderr else "Unknown error"
            logger.error(f"âŒ Crawler process failed (code {process.returncode}): {error_msg}")
            await update_job_status(job_id, status="failed", errors=[f"Exit code {process.returncode}", error_msg])
        else:
            logger.info(f"âœ… Crawler process finished successfully.")

    except Exception as e:
        logger.error(f"ğŸ”¥ Failed to launch isolated crawler: {e}", exc_info=True)
        await update_job_status(job_id, status="failed", errors=[str(e)])
    finally:
        job_manager.remove_job(job_id)

# --- LLM & Chemical Enrichment ---

async def refine_drug_data_with_llm(title: str, text_content: str, raw_status: Optional[str] = None) -> dict:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ADC ì •ë³´ ì¶”ì¶œ ë° PubChem ë³´ê°•"""
    try:
        llm_refiner = ChatOpenAI(
            model=settings.FAST_LLM,
            temperature=0,
            api_key=settings.OPENAI_API_KEY
        )
        
        system_prompt = """You are an expert Clinical & Pharmaceutical Data Analyst. 
        Extract structured ADC (Antibody-Drug Conjugate) information.
        Output JSON format:
        {
            "drug_name": {"value": "DS-8201a", "evidence": "..."},
            "target": {"value": "HER2", "evidence": "..."},
            "payload": {"value": "DXd", "evidence": "..."},
            "linker": {"value": "Tetrapeptide-based", "evidence": "..."},
            "outcome_type": {"value": "Success", "evidence": "..."},
            "smiles_code": {"value": null, "evidence": null}
        }"""
        
        user_prompt = f"Title: {title}\nContent: {text_content}\nStatus: {raw_status}"
        prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("user", user_prompt)])
        chain = prompt | llm_refiner
        response = await chain.ainvoke({})
        content = response.content.strip()
        if "```json" in content: content = content.split("```json")[1].split("```")[0]
        data = json.loads(content)
        
        # SMILES ë³´ê°• (PubChem Resolver)
        drug_name = data.get("drug_name", {}).get("value")
        if drug_name and not data.get("smiles_code", {}).get("value"):
            smiles = chemical_resolver.fetch_verified_smiles(drug_name)
            if smiles:
                data["smiles_code"] = {"value": smiles, "evidence": "Verified via PubChem API"}
        
        return data
    except Exception as e:
        logger.error(f"LLM Refine Error: {e}")
        return {"drug_name": {"value": "Unknown"}}

# --- ClinicalTrials.gov Worker (DB Status & Paging) ---

async def process_clinical_trials_data(job_id: str, max_records: int = 1000):
    """ClinicalTrials.gov ëŒ€ëŸ‰ ìˆ˜ì§‘ ì›Œì»¤ (BulkImporter í†µí•©)"""
    from app.services.bulk_importer import BulkImporter
    
    importer = BulkImporter()
    # ê¸°ë³¸ì ìœ¼ë¡œ daily ëª¨ë“œë¡œ ë™ì‘ (í•„ìš” ì‹œ mode íŒŒë¼ë¯¸í„° í™•ì¥ ê°€ëŠ¥)
    await importer.run_import(job_id, max_studies=max_records, mode="daily")

async def process_single_study_simple(study: dict) -> bool:
    """ë‹¨ìˆœí™”ëœ ìŠ¤í„°ë”” ì €ì¥ (LLM ì—†ì´)"""
    try:
        protocol = study.get("protocolSection", {})
        id_module = protocol.get("identificationModule", {})
        nct_id = id_module.get("nctId")
        if not nct_id: return False
        
        existing = supabase.table("golden_set_library").select("id").eq("properties->>nct_id", nct_id).execute()
        if existing.data: return False

        title = id_module.get("officialTitle") or id_module.get("briefTitle", "No Title")
        status_module = protocol.get("statusModule", {})
        
        new_entry = {
            "name": title[:100],
            "category": "clinical_trial",
            "description": title,
            "properties": {"nct_id": nct_id, "phase": status_module.get("phase"), "status": status_module.get("overallStatus")},
            "status": "draft",
            "outcome_type": "Unknown",
            "enrichment_source": "clinical_trials_simple"
        }
        supabase.table("golden_set_library").insert(new_entry).execute()
        return True
    except Exception as e:
        logger.error(f"Study save error {study.get('protocolSection', {}).get('identificationModule', {}).get('nctId')}: {e}")
        return False

async def process_single_study(study: dict, job_id: str) -> bool:
    """LLM í¬í•¨ ìŠ¤í„°ë”” ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
    try:
        protocol = study.get("protocolSection", {})
        id_module = protocol.get("identificationModule", {})
        nct_id = id_module.get("nctId")
        if not nct_id: return False
        
        existing = supabase.table("golden_set_library").select("id").eq("properties->>nct_id", nct_id).execute()
        if existing.data: return False

        refined = await refine_drug_data_with_llm(
            title=id_module.get("officialTitle", "No Title"),
            text_content=str(protocol.get("descriptionModule", {})),
            raw_status=protocol.get("statusModule", {}).get("overallStatus")
        )
        
        new_entry = {
            "name": refined.get("drug_name", {}).get("value", "Unknown"),
            "category": "clinical_trial",
            "description": id_module.get("officialTitle", "No Title"),
            "properties": {"nct_id": nct_id, "ai_analysis": refined},
            "smiles_code": refined.get("smiles_code", {}).get("value"),
            "status": "draft",
            "outcome_type": refined.get("outcome_type", {}).get("value", "Unknown"),
            "enrichment_source": "clinical_trials_v2_db_status"
        }
        supabase.table("golden_set_library").insert(new_entry).execute()
        return True
    except Exception:
        return False

# --- PubMed Worker (DB Status & Paging) ---

async def process_pubmed_data(job_id: str, max_records: int = 500, mode: str = "daily"):
    """PubMed ëŒ€ëŸ‰ ìˆ˜ì§‘ ì›Œì»¤ (DB ê¸°ë°˜)"""
    await update_job_status(job_id, status="running")
    drafted = 0
    errors = []
    
    # ëª¨ë“œì— ë”°ë¥¸ ì„¤ì •
    days_back = 2 if mode == "daily" else 3650 # Daily: 2ì¼, Full: 10ë…„ (ê¸°ë³¸ê°’)
    
    # Smart Trawl ì „ëµ: 2015ë…„ ì´í›„ ë°ì´í„°ë§Œ (Full Load ì‹œ)
    mindate = "2015/01/01" if mode == "full" else None
    
    if mode == "full":
        max_records = 2000 # Full load ì‹œ ë” ë§ì´ ìˆ˜ì§‘
    
    try:
        loop = asyncio.get_event_loop()
        
        # Smart Trawl Query Construction
        # 1. Target Keywords
        target_terms = '(Antibody-Drug Conjugate[Title/Abstract] OR ADC[Title/Abstract] OR Immunoconjugate[Title/Abstract])'
        # 2. Context Keywords (Safety Guard)
        context_terms = '(Cancer[Title/Abstract] OR Tumor[Title/Abstract] OR Oncology[Title/Abstract] OR Neoplasms[MeSH Terms])'
        # 3. Final Query
        search_term = f"{target_terms} AND {context_terms}"
        
        id_list = await loop.run_in_executor(None, lambda: fetch_pubmed_ids(search_term, max_records, days_back, mindate))
        await update_job_status(job_id, records_found=len(id_list))
        
        if not id_list:
            await update_job_status(job_id, status="completed", completed_at=datetime.utcnow().isoformat())
            return

        for i in range(0, len(id_list), 10):
            if await is_cancelled(job_id):
                await update_job_status(job_id, status="stopped")
                return

            batch_ids = id_list[i:i+10]
            papers = await loop.run_in_executor(None, fetch_pubmed_details, batch_ids)
            
            if 'PubmedArticle' in papers:
                tasks = [process_single_article(article, job_id) for article in papers['PubmedArticle']]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for r in results:
                    if isinstance(r, Exception): errors.append(str(r))
                    elif r: drafted += 1
                
                await update_job_status(job_id, records_drafted=drafted, errors=errors[:20])

        await update_job_status(job_id, status="completed", completed_at=datetime.utcnow().isoformat())
    except Exception as e:
        logger.error(f"PubMed Worker Error: {e}")
        await update_job_status(job_id, status="failed", errors=[str(e)])

async def process_single_article(article: dict, job_id: str) -> bool:
    """PubMed ë…¼ë¬¸ì„ knowledge_baseì— ì €ì¥ (RAGìš© ì§€ì‹ ë² ì´ìŠ¤)"""
    try:
        medline = article['MedlineCitation']
        article_data = medline['Article']
        title = str(article_data.get('ArticleTitle', 'No Title'))
        
        # ì¤‘ë³µ ì²´í¬ (title ê¸°ì¤€)
        existing = supabase.table("knowledge_base").select("id").eq("title", title).execute()
        if existing.data: 
            return False
        
        # ì´ˆë¡ ì¶”ì¶œ
        abstract_texts = article_data.get('Abstract', {}).get('AbstractText', [])
        if isinstance(abstract_texts, list):
            content = " ".join([str(t) for t in abstract_texts])
        else:
            content = str(abstract_texts) if abstract_texts else ""
        
        # PMID ì¶”ì¶œ
        pmid = str(medline.get('PMID', ''))
        
        # ì €ë„ ì •ë³´
        journal = article_data.get('Journal', {}).get('Title', '')
        
        # knowledge_base ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì €ì¥
        new_kb = {
            "source_type": "PubMed",
            "title": title[:500],
            "summary": f"PMID: {pmid} | Journal: {journal}" if pmid else "",
            "content": content if content else "No abstract available.",
            "relevance_score": None,
            "source_tier": 1,  # PubMed = Tier 1 (í•™ìˆ  ë…¼ë¬¸)
            "ai_reasoning": None,
            "rag_status": "pending"
        }
        
        supabase.table("knowledge_base").insert(new_kb).execute()
        logger.info(f"âœ… Saved PubMed: {title[:50]}...")
        return True
    except Exception as e:
        logger.error(f"PubMed save error: {e}")
        return False

def fetch_pubmed_ids(term: str, max_results: int, days_back: int = 7, mindate: Optional[str] = None):
    # ë‚ ì§œ í•„í„°ë§
    end_date = datetime.now()
    
    # mindateê°€ ëª…ì‹œë˜ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì•„ë‹ˆë©´ days_back ì‚¬ìš©
    if mindate:
        start_date_str = mindate
    else:
        start_date = end_date - timedelta(days=days_back)
        start_date_str = start_date.strftime("%Y/%m/%d")
    
    handle = Entrez.esearch(
        db="pubmed", 
        term=term, 
        retmax=max_results, 
        sort="pub_date",
        datetype="pdat",
        mindate=start_date_str,
        maxdate=end_date.strftime("%Y/%m/%d")
    )
    record = Entrez.read(handle)
    handle.close()
    return record.get("IdList", [])

def fetch_pubmed_details(id_list: List[str]):
    handle = Entrez.efetch(db="pubmed", id=",".join(id_list), retmode="xml")
    records = Entrez.read(handle)
    handle.close()
    return records

# --- API Endpoints ---

@router.post("/sync/clinical", response_model=SyncJobResponse)
async def sync_clinical_trials(background_tasks: BackgroundTasks):
    job_id = f"sync_clinical_{uuid4().hex[:8]}"
    data = {"id": job_id, "status": "queued", "source": "clinical_trials", "started_at": datetime.utcnow().isoformat()}
    supabase.table("sync_jobs").insert(data).execute()
    background_tasks.add_task(process_clinical_trials_data, job_id)
    return SyncJobResponse(job_id=job_id, status="queued", message="ClinicalTrials.gov sync started.")

@router.post("/sync/pubmed", response_model=SyncJobResponse)
async def sync_pubmed(background_tasks: BackgroundTasks, mode: str = "daily"):
    job_id = f"sync_pubmed_{uuid4().hex[:8]}"
    data = {"id": job_id, "status": "queued", "source": "pubmed", "started_at": datetime.utcnow().isoformat()}
    supabase.table("sync_jobs").insert(data).execute()
    background_tasks.add_task(process_pubmed_data, job_id, 500, mode) # Pass mode
    return SyncJobResponse(job_id=job_id, status="queued", message=f"PubMed sync started (mode: {mode}).")

@router.post("/sync/openfda", response_model=SyncJobResponse)
async def sync_openfda(background_tasks: BackgroundTasks, mode: str = "daily", limit: int = 100):
    job_id = f"sync_openfda_{uuid4().hex[:8]}"
    data = {"id": job_id, "status": "queued", "source": "openfda", "started_at": datetime.utcnow().isoformat()}
    supabase.table("sync_jobs").insert(data).execute()
    background_tasks.add_task(openfda_service.sync_to_db, job_id, mode, limit)
    return SyncJobResponse(job_id=job_id, status="queued", message=f"OpenFDA sync started (mode: {mode}, limit: {limit}).")

@router.post("/crawler/creative-biolabs/run", response_model=SyncJobResponse)
async def run_creative_biolabs_crawler(background_tasks: BackgroundTasks, category: str = "ADC Cytotoxin", limit: int = 10):
    """Creative Biolabs Stealth Crawler ì‹¤í–‰ (ê²©ë¦¬ í”„ë¡œì„¸ìŠ¤ ë°©ì‹)"""
    job_id = f"crawl_cb_{uuid4().hex[:8]}"
    
    # DBì— ì‘ì—… ê¸°ë¡
    data = {
        "id": job_id, 
        "status": "running", 
        "source": "creative_biolabs", 
        "started_at": datetime.now(timezone.utc).isoformat()
    }
    supabase.table("sync_jobs").insert(data).execute()
    
    job_manager.add_job(job_id)
    background_tasks.add_task(run_isolated_crawler, "creative_biolabs", category, limit, job_id)
    
    return SyncJobResponse(job_id=job_id, status="queued", message="Creative Biolabs crawler started (Isolated Process).")

@router.post("/crawler/ambeed/run", response_model=SyncJobResponse)
async def run_ambeed_crawler(background_tasks: BackgroundTasks, category: str = "all", limit: int = 10, start_page: int = 1):
    """Ambeed Stealth Crawler ì‹¤í–‰ (ê²©ë¦¬ í”„ë¡œì„¸ìŠ¤ ë°©ì‹)"""
    job_id = f"crawl_ambeed_{uuid4().hex[:8]}"
    
    data = {
        "id": job_id, 
        "status": "running", 
        "source": "ambeed", 
        "started_at": datetime.now(timezone.utc).isoformat()
    }
    supabase.table("sync_jobs").insert(data).execute()
    
    job_manager.add_job(job_id)
    background_tasks.add_task(run_isolated_crawler, "ambeed", category, limit, job_id, start_page)
    
    return SyncJobResponse(job_id=job_id, status="queued", message=f"Ambeed crawler started (Isolated Process) from page {start_page}.")

@router.get("/sync/{job_id}")
async def get_sync_status(job_id: str):
    job = await get_job_from_db(job_id)
    if not job: raise HTTPException(status_code=404, detail="Job not found")
    
    # Zombie Check
    if job.get("status") == "running" and not job_manager.is_active(job_id):
        started_at_str = job.get("started_at")
        try:
            # Handle ISO format with 'Z' or offset
            if started_at_str.endswith('Z'):
                started_at = datetime.fromisoformat(started_at_str[:-1]).replace(tzinfo=timezone.utc)
            else:
                started_at = datetime.fromisoformat(started_at_str)
                if started_at.tzinfo is None:
                    started_at = started_at.replace(tzinfo=timezone.utc)

            if (datetime.now(timezone.utc) - started_at).total_seconds() > 120: # 2ë¶„ìœ¼ë¡œ ìƒí–¥
                logger.warning(f"ğŸ§Ÿ Zombie Job Detected: {job_id}. Marking as failed.")
                await update_job_status(job_id, status="failed", errors=["Zombie process detected (timeout)"])
                job["status"] = "failed"
        except Exception as e:
            logger.error(f"Error parsing started_at: {e}")
            
    return job

@router.post("/sync/{job_id}/stop")
async def stop_sync_job(job_id: str):
    await update_job_status(job_id, cancel_requested=True)
    return {"message": "Stop request sent to DB."}

@router.post("/workers/reset")
async def reset_all_workers():
    """ëª¨ë“  ì›Œì»¤ ê°•ì œ ì¤‘ë‹¨ ë° ì ê¸ˆ í•´ì œ"""
    try:
        # 1. ëª¨ë“  ì ê¸ˆ í•´ì œ
        supabase.table("job_locks").delete().neq("id", "00000000-0000-0000-0000-000000000000").execute()
        
        # 2. ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ìƒíƒœ ë³€ê²½
        # 2. ì‹¤í–‰ ì¤‘ì¸ ì‘ì—… ìƒíƒœ ë³€ê²½ (ë¨¼ì € ì·¨ì†Œ ìš”ì²­ í”Œë˜ê·¸ ì„¤ì •)
        supabase.table("sync_jobs").update({
            "cancel_requested": True
        }).eq("status", "running").execute()

        # 3. ì ì‹œ ëŒ€ê¸° í›„ ê°•ì œ ì¢…ë£Œ ì²˜ë¦¬ (ì˜µì…˜: ì›Œì»¤ê°€ ìŠ¤ìŠ¤ë¡œ ì¢…ë£Œí•˜ë„ë¡ ìœ ë„í•˜ê±°ë‚˜, ì—¬ê¸°ì„œ ê°•ì œë¡œ statusë¥¼ ë°”ê¿ˆ)
        # ì—¬ê¸°ì„œëŠ” ì¦‰ì‹œ statusë¥¼ stoppedë¡œ ë°”ê¾¸ì§€ë§Œ, ì›Œì»¤ ë¡œì§ì—ì„œ cancel_requestedë¥¼ í™•ì¸í•˜ê³  ì¢…ë£Œí•˜ë„ë¡ í•¨
        supabase.table("sync_jobs").update({
            "status": "stopped",
            "errors": ["Force stopped by admin"],
            "completed_at": datetime.utcnow().isoformat()
        }).eq("status", "running").execute()
        
        return {"message": "All workers have been reset and locks released."}
    except Exception as e:
        logger.error(f"Failed to reset workers: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# --- Bulk Import & AI Refiner Endpoints ---

@router.post("/bulk/import", response_model=SyncJobResponse)
async def run_bulk_import(background_tasks: BackgroundTasks, max_studies: int = 5000, mode: str = "daily"):
    """
    ClinicalTrials.gov ì „ì²´ ë¤í”„ì—ì„œ ADC ë°ì´í„° ì¼ê´„ ì„í¬íŠ¸
    mode: 'daily' (ê¸°ë³¸ê°’) ë˜ëŠ” 'full' (ì „ì²´ ì ì¬)
    """
    from app.services.bulk_importer import BulkImporter
    
    # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
    if not await job_lock.acquire("bulk_import"):
        raise HTTPException(
            status_code=409, 
            detail="ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ Bulk Import ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
        )
    
    job_id = f"bulk_import_{uuid4().hex[:8]}"
    data = {
        "id": job_id, 
        "status": "queued", 
        "source": "clinical_trials_bulk", 
        "started_at": datetime.utcnow().isoformat()
    }
    supabase.table("sync_jobs").insert(data).execute()
    
    async def run_with_lock_release(importer, job_id, max_studies, mode):
        try:
            await importer.run_import(job_id, max_studies, mode=mode)
        finally:
            await job_lock.release("bulk_import")
    
    importer = BulkImporter()
    background_tasks.add_task(run_with_lock_release, importer, job_id, max_studies, mode)
    
    return SyncJobResponse(
        job_id=job_id, 
        status="queued", 
        message=f"Bulk import started (mode: {mode}, max {max_studies} studies)."
    )

@router.post("/refiner/run", response_model=SyncJobResponse)
async def run_ai_refiner(background_tasks: BackgroundTasks, max_records: int = 50):
    """ë¯¸ì •ì œ ë ˆì½”ë“œ LLM ë¶„ì„ ë° SMILES ë³´ê°•"""
    from app.services.ai_refiner import ai_refiner
    
    # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
    if not await job_lock.acquire("ai_refiner"):
        raise HTTPException(
            status_code=409, 
            detail="ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ AI Refiner ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
        )
    
    job_id = f"ai_refiner_{uuid4().hex[:8]}"
    data = {
        "id": job_id, 
        "status": "queued", 
        "source": "ai_refiner", 
        "started_at": datetime.utcnow().isoformat()
    }
    supabase.table("sync_jobs").insert(data).execute()
    
    async def run_with_lock_release(job_id, max_records):
        try:
            await ai_refiner.process_pending_records(job_id, max_records)
        finally:
            await job_lock.release("ai_refiner")
    
    background_tasks.add_task(run_with_lock_release, job_id, max_records)
    
    return SyncJobResponse(
        job_id=job_id, 
        status="queued", 
        message=f"AI Refiner started (max {max_records} records)."
    )

@router.post("/sync/pubmed-knowledge", response_model=SyncJobResponse)
async def sync_pubmed_knowledge(
    background_tasks: BackgroundTasks, 
    batch_size: int = 100,
    mode: str = "incremental"
):
    """
    PubMed ê¸°ë°˜ ADC Knowledge Base êµ¬ì¶•
    golden_set_libraryì—ì„œ ì•½ë¬¼ ì¶”ì¶œ â†’ PubMed ê²€ìƒ‰ â†’ Gemini ë¶„ì„ â†’ knowledge_base ì €ì¥
    
    mode: 'incremental' (ë°°ì¹˜ í¬ê¸°ë§Œí¼) ë˜ëŠ” 'full' (ì „ì²´)
    """
    from app.services.pubmed_knowledge_service import PubMedKnowledgeService
    
    # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
    if not await job_lock.acquire("pubmed_knowledge"):
        raise HTTPException(
            status_code=409, 
            detail="ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ PubMed Knowledge ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤."
        )
    
    job_id = f"pubmed_knowledge_{uuid4().hex[:8]}"
    data = {
        "id": job_id, 
        "status": "queued", 
        "source": "pubmed_knowledge", 
        "started_at": datetime.utcnow().isoformat()
    }
    supabase.table("sync_jobs").insert(data).execute()
    
    async def run_with_lock_release(job_id, batch_size, mode):
        try:
            service = PubMedKnowledgeService()
            await service.run_batch(job_id, batch_size, mode)
        finally:
            await job_lock.release("pubmed_knowledge")
    
    background_tasks.add_task(run_with_lock_release, job_id, batch_size, mode)
    
    return SyncJobResponse(
        job_id=job_id, 
        status="queued", 
        message=f"PubMed Knowledge sync started (batch: {batch_size}, mode: {mode})."
    )

@router.get("/refiner/pending")
async def get_pending_count():
    """ë¯¸ì •ì œ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ"""
    try:
        res = supabase.table("golden_set_library")\
            .select("count", count="exact")\
            .eq("ai_refined", False)\
            .execute()
        return {"pending_count": res.count}
    except Exception as e:
        return {"pending_count": 0, "error": str(e)}

@router.get("/health")
async def health():
    res = supabase.table("sync_jobs").select("count", count="exact").eq("status", "running").execute()
    return {"status": "healthy", "active_jobs": res.count}
