import asyncio
import random
import logging
import json
import time
import subprocess
import os
import re

# Force gRPC to be less aggressive
os.environ["GRPC_TYPE_CHECK"] = "0"
from typing import Dict, List, Optional, Any
from datetime import datetime

from playwright.async_api import async_playwright, Page, BrowserContext, TimeoutError as PlaywrightTimeoutError
from fake_useragent import UserAgent
import aiohttp
import google.generativeai as genai
from rdkit import Chem # SMILES Validation

from app.core.config import settings
from app.core.supabase import supabase
from app.services.ai_refiner import ai_refiner
from app.services.rag_service import rag_service
from app.services.job_lock import job_lock

logger = logging.getLogger(__name__)

class AmbeedCrawler:
    """
    Ambeed Full-Scale Crawler (V4 SMILES Specialized)
    - Precise Detail Page Parsing for SMILES
    - RDKit Validation
    - PubChem Fallback (via CAS No)
    """
    
    BASE_URL = "https://www.ambeed.com"
    CATEGORIES = {
        "ADC Toxins": "https://www.ambeed.com/adc-toxins.html",
        "ADC Linker": "https://www.ambeed.com/adc-linkers.html"
    }
    
    def __init__(self):
        self.ua = UserAgent()
        self.global_semaphore = asyncio.Semaphore(1)
        self.model = None

    def _get_model(self):
        if not self.model:
            genai.configure(api_key=settings.GOOGLE_API_KEY)
            # ì‚¬ì¥ë‹˜ ì§€ì‹œ: ìµœì‹  2.0 Flash ëª¨ë¸ë¡œ ì—…ê·¸ë ˆì´ë“œ
            self.model = genai.GenerativeModel('gemini-2.0-flash')
        return self.model

    async def _init_browser(self, p) -> BrowserContext:
        try:
            browser = await p.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
            )
            context = await browser.new_context(user_agent=self.ua.random)
            
            # --- RESOURCE BLOCKING (Critical for Speed) ---
            async def route_intercept(route):
                if route.request.resource_type in ["image", "media", "font", "stylesheet"]:
                    await route.abort()
                else:
                    await route.continue_()

            await context.route("**/*", route_intercept)
            return context
        except Exception as e:
            logger.error(f"ğŸ”¥ Ambeed Browser Launch Failed: {e}")
            raise e

    def validate_smiles(self, smiles: str) -> bool:
        """RDKitì„ ì‚¬ìš©í•œ SMILES ìœ íš¨ì„± ê²€ì¦"""
        if not smiles or not isinstance(smiles, str): return False
        try:
            mol = Chem.MolFromSmiles(smiles)
            return mol is not None
        except: 
            return False

    async def fetch_pubchem_smiles_by_cas(self, cas: str) -> Optional[str]:
        """CAS Noë¥¼ ì´ìš©í•´ PubChemì—ì„œ SMILES ì¶”ë¡  (Fallback)"""
        if not cas: return None
        logger.info(f"ğŸ§¬ Attempting PubChem fallback for CAS: {cas}")
        try:
            url = f"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{cas}/property/IsomericSMILES/JSON"
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    if response.status == 200:
                        data = await response.json()
                        smiles = data['PropertyTable']['Properties'][0].get('IsomericSMILES')
                        if self.validate_smiles(smiles):
                            logger.info(f"âœ¨ Successfully found SMILES on PubChem for {cas}")
                            return smiles
        except Exception as e:
            logger.warning(f"âš ï¸ PubChem fallback failed for {cas}: {e}")
        return None

    async def crawl_category(self, category_name: str, base_url: str, limit: int = 10, job_id: str = None, start_page: int = 1, batch_size: int = 2) -> int:
        logger.info(f"ğŸš€ [AMBEED SMILES CRAWL] {category_name} (Start Page: {start_page}, Batch: {batch_size})")
        from app.api.scheduler import update_job_status, get_job_from_db, is_cancelled
        
        count = 0
        current_start_page = start_page
        
        # 1. Offset ê´€ë¦¬: DBì—ì„œ ë§ˆì§€ë§‰ ì§„í–‰ í˜ì´ì§€ ì¡°íšŒ (start_pageê°€ 1ì¼ ë•Œë§Œ DB ì¡°íšŒ)
        if job_id and current_start_page == 1:
            job_data = await get_job_from_db(job_id)
            if job_data and job_data.get("last_processed_page"):
                current_start_page = job_data["last_processed_page"] + 1
                logger.info(f"â­ï¸ Resuming from page {current_start_page}")

        page_num = current_start_page
        batch_data = []
        
        async with async_playwright() as p:
            try:
                context = await self._init_browser(p)
                page = await context.new_page()
                
                while count < limit:
                    if job_id and await is_cancelled(job_id):
                        logger.info(f"ğŸ›‘ Job {job_id} cancelled.")
                        break

                    separator = "&" if "?" in base_url else "?"
                    url = base_url if page_num == 1 else f"{base_url}{separator}page={page_num}"
                    logger.info(f"ğŸŒ [PAGE {page_num}] ë‹¤ìŒ í˜ì´ì§€ ì ‘ì† ì¤‘: {url} (í˜„ì¬ ìˆ˜ì§‘ëŸ‰: {count}/{limit})")
                    
                    try:
                        # í˜ì´ì§€ ì ‘ì† ì‹œë„ (íƒ€ì„ì•„ì›ƒ ê°•í™” ë° ì¬ì‹œë„ ë¡œì§ì€ ìƒëµí•˜ë˜ í™•ì‹¤íˆ ëŒ€ê¸°)
                        response = await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                        await asyncio.sleep(3) # ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¶©ë¶„íˆ ë¶€ì—¬
                        
                        # ìƒí’ˆ ëª©ë¡ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
                        products = await page.evaluate("""
                            () => {
                                // 1. í‘œì¤€ ì œí’ˆ ì•„ì´í…œ í´ë˜ìŠ¤
                                let items = Array.from(document.querySelectorAll('.product-item, .item, .product-info-main, .product-item-info'));
                                
                                // 2. ë§Œì•½ ì—†ë‹¤ë©´ ëª¨ë“  ì œí’ˆ ìƒì„¸ ë§í¬ ì°¾ê¸°
                                if (items.length === 0) {
                                    return Array.from(document.querySelectorAll('a[href*="/products/"]'))
                                        .map(a => ({ href: a.href, cat_no: null }))
                                        .filter((v, i, a) => a.findIndex(t => t.href === v.href) === i); // ì¤‘ë³µ ì œê±°
                                }

                                return items.map(el => {
                                    const linkEl = el.querySelector('a[href*="/products/"], a[href*="/record/"]');
                                    const catNoEl = el.innerText.match(/Cat No:?\s*([A-Z0-9-]+)/i);
                                    return {
                                        href: linkEl ? linkEl.href : null,
                                        cat_no: catNoEl ? catNoEl[1] : null
                                    };
                                }).filter(p => p.href);
                            }
                        """)
                        
                        if not products:
                            # <a> íƒœê·¸ë§Œì´ë¼ë„ ë’¤ì ¸ì„œ ì°¾ê¸°
                            links = await page.evaluate("""
                                () => Array.from(document.querySelectorAll('a[href*="/products/"]'))
                                    .map(a => a.href)
                                    .filter(href => href.includes('.html'))
                            """)
                            products = [{"href": link, "cat_no": None} for link in set(links)]

                        if not products:
                            logger.warning(f"âš ï¸ [PAGE {page_num}] ìƒí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 1í˜ì´ì§€ë¥¼ ë” ê±´ë„ˆë›°ì–´ ë´…ë‹ˆë‹¤.")
                            page_num += 1
                            if page_num > start_page + 50: # ë„ˆë¬´ ë§ì´ ë¹„ì–´ìˆìœ¼ë©´ ì¢…ë£Œ
                                logger.error("ğŸ ì—°ì†ëœ ë¹ˆ í˜ì´ì§€ ë°œìƒìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                                break
                            continue
                        
                        logger.info(f"ğŸ“¦ [PAGE {page_num}] {len(products)}ê°œì˜ ìƒí’ˆì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

                        for prod in products:
                            if count >= limit: break
                            
                            link = prod["href"]
                            cat_no = prod["cat_no"] or link.split('/')[-1].replace('.html', '')
                            
                            logger.info(f"ğŸ”„ [PROCESS] Item: {cat_no} (Page {page_num})")

                            res = await self._process_single_product(context, link, category_name)
                            if res:
                                final_item = await self._enrich_and_prepare_item(res)
                                if final_item:
                                    batch_data.append(final_item)
                                    count += 1
                                    
                                    if len(batch_data) >= batch_size:
                                        logger.info(f"ğŸ’¾ [ë‹¨ê³„ 3] {batch_size}ê°œ ë„ë‹¬! DB ì €ì¥ ì‹œë„ (Page {page_num})")
                                        await self._save_batch(batch_data)
                                        batch_data = []
                                        if job_id:
                                            await update_job_status(job_id, records_drafted=count, last_processed_page=page_num)
                                else:
                                    logger.warning(f"âš ï¸ [SKIP] Enrichment failed for {cat_no}")
                            else:
                                logger.warning(f"âš ï¸ [SKIP] Processing failed for {cat_no}")

                        # í•œ í˜ì´ì§€ ì²˜ë¦¬ê°€ ëë‚˜ë©´ "ë¬´ì¡°ê±´" í˜ì´ì§€ ë²ˆí˜¸ ì¦ê°€
                        logger.info(f"âœ… [PAGE {page_num}] ì²˜ë¦¬ ì™„ë£Œ. ë‹¤ìŒ í˜ì´ì§€({page_num + 1})ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                        page_num += 1
                        
                        if job_id:
                            await update_job_status(job_id, last_processed_page=page_num)

                    except Exception as e:
                        logger.error(f"âŒ Error on page {page_num}: {e}")
                        page_num += 1 # ì—ëŸ¬ ë‚˜ë„ ë‹¤ìŒ í˜ì´ì§€ ì‹œë„
                        await asyncio.sleep(5)
                
                # ë‚¨ì€ ë°ì´í„° ì €ì¥
                if batch_data:
                    logger.info(f"ğŸ’¾ [Final Batch] ë‚¨ì€ {len(batch_data)}ê°œ ì €ì¥ ì‹œë„")
                    await self._save_batch(batch_data)
                    if job_id:
                        await update_job_status(job_id, records_drafted=count)
            finally: 
                await context.close()
        return count

    async def _enrich_and_prepare_item(self, raw_data):
        """ì €ì¥ ì „ ë°ì´í„° ë³´ê°• ë° ê²€ì¦ (Upsertìš© ë°ì´í„° ìƒì„±)"""
        try:
            # 1. SMILES ê²€ì¦ ë° ë³´ì™„
            smiles = raw_data.get("smiles_code")
            is_valid = self.validate_smiles(smiles)
            
            if not is_valid:
                logger.warning(f"âš ï¸ Invalid SMILES for {raw_data['ambeed_cat_no']}. Trying PubChem...")
                fallback_smiles = await self.fetch_pubchem_smiles_by_cas(raw_data.get("cas_number"))
                if fallback_smiles:
                    smiles = fallback_smiles
                    is_valid = True
            
            # SMILES í•„ìˆ˜ ì²´í¬ ë¡œê·¸
            if not is_valid:
                logger.error(f"âŒ SMILES MISSING for {raw_data['ambeed_cat_no']} after fallback.")
            
            # 2. AI ì •ì œ (Gemini 2.0 Flash) - 3ë‹¨ SMILES ë¶„ë¦¬ ë¡œì§ íƒ‘ì¬
            ai_data = await self._enrich_with_gemini(raw_data, smiles)
            
            # DB ì»¬ëŸ¼ ëˆ„ë½ ëŒ€ë¹„: properties ì•ˆì— ìƒì„¸ SMILES ì •ë³´ ë°±ì—…
            extended_properties = raw_data.get("properties", {})
            if isinstance(extended_properties, dict):
                extended_properties.update({
                    "payload_smiles": ai_data.get("payload_smiles"),
                    "linker_smiles": ai_data.get("linker_smiles"),
                    "full_smiles": ai_data.get("full_smiles") or smiles
                })

            final_data = {
                "ambeed_cat_no": raw_data["ambeed_cat_no"],
                "product_name": raw_data["product_name"],
                "product_url": raw_data["product_url"],
                "category": raw_data["category"],
                "source_name": "Ambeed",
                "smiles_code": smiles if is_valid else None,
                # ì•„ë˜ ì»¬ëŸ¼ë“¤ì€ DBì— ì¶”ê°€ë˜ì–´ì•¼ ì‘ë™í•©ë‹ˆë‹¤.
                "payload_smiles": ai_data.get("payload_smiles"),
                "linker_smiles": ai_data.get("linker_smiles"),
                "full_smiles": ai_data.get("full_smiles") or smiles,
                "cas_number": raw_data.get("cas_number"),
                "target": ai_data.get("target"),
                "summary": ai_data.get("summary"),
                "properties": extended_properties, # ë°±ì—…ìš©ìœ¼ë¡œ JSON ì•ˆì—ë„ ì €ì¥
                "crawled_at": raw_data["crawled_at"],
                "ai_refined": True
            }
            
            # ì„ë² ë”© ìƒì„± (ì‹¤íŒ¨í•´ë„ ì €ì¥ì€ ë˜ì–´ì•¼ í•¨)
            try:
                embed_text = f"{final_data['product_name']} {final_data.get('smiles_code') or ''} {final_data.get('target') or ''}"
                # 768 ì°¨ì›ì¸ì§€ í™•ì¸ ë“± RAG ì„œë¹„ìŠ¤ ë‚´ë¶€ ë¡œì§ì— ì˜ì¡´
                embedding = await rag_service.generate_embedding(embed_text)
                if embedding:
                    final_data["embedding"] = embedding
            except Exception as e:
                logger.warning(f"âš ï¸ Embedding failed for {final_data['ambeed_cat_no']}, proceeding without it: {e}")
            
            return final_data
        except Exception as e:
            logger.error(f"ğŸ”¥ [ì¹˜ëª…ì  ì—ëŸ¬] ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„ ì‹¤íŒ¨ ({raw_data.get('ambeed_cat_no')}): {e}", exc_info=True)
            return None

    async def _save_batch(self, items: List[Dict]):
        """ë°°ì¹˜ UPSERT ì‹¤í–‰"""
        try:
            if not items: return False
            
            # DB ì—°ê²° ìƒíƒœ ì²´í¬ (MockClient ë°©ì§€)
            if hasattr(supabase, "is_mock") and supabase.is_mock:
                logger.error("ğŸ”¥ [CRITICAL] Supabase is running as a MOCK client. Check .env file!")
                return False

            logger.info(f"ğŸ“¤ Supabase UPSERT ìš”ì²­ ì¤‘... ({len(items)}ê±´)")
            res = supabase.table("commercial_reagents").upsert(items, on_conflict="ambeed_cat_no").execute()
            if res.data:
                logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ! ({len(res.data)}ê±´ ë°˜ì˜ë¨)")
                return True
            else:
                logger.error("âŒ DB ì €ì¥ ì‹¤íŒ¨: ì‘ë‹µ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
        except Exception as e:
            logger.error(f"ğŸ”¥ [DB ì¹˜ëª…ì  ì—ëŸ¬] Batch save failed: {e}", exc_info=True)
            return False

    async def _enrich_and_save_single(self, raw_data):
        # This is now handled by _enrich_and_prepare_item and _save_batch
        pass

    async def _process_single_product(self, context, url, category):
        async with self.global_semaphore:
            page = await context.new_page()
            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                title = await page.title()
                
                # ìƒì„¸ í˜ì´ì§€ ì •ë°€ íŒŒì‹± (SMILES ë¼ë²¨ íƒ€ê²ŸíŒ…)
                extracted = await page.evaluate("""
                    () => {
                        const results = { cat_no: '', cas_no: '', smiles: '' };
                        const tds = Array.from(document.querySelectorAll('td, th, span, div'));
                        
                        // SMILES ì°¾ê¸°
                        for (let i = 0; i < tds.length; i++) {
                            const text = tds[i].innerText;
                            if (text.includes('SMILES')) {
                                // ë‹¤ìŒ í˜•ì œ ìš”ì†Œë‚˜ ë¶€ëª¨ì˜ í…ìŠ¤íŠ¸ì—ì„œ SMILES ì¶”ì¶œ ì‹œë„
                                let smilesVal = tds[i+1]?.innerText || tds[i].parentElement.innerText.split('SMILES')[1];
                                if (smilesVal) results.smiles = smilesVal.replace(':', '').trim();
                            }
                            if (text.includes('CAS No')) {
                                let casVal = tds[i+1]?.innerText || tds[i].parentElement.innerText.split('CAS No')[1];
                                if (casVal) results.cas_no = casVal.replace(':', '').trim();
                            }
                            if (text.includes('Catalog No')) {
                                let catVal = tds[i+1]?.innerText || tds[i].parentElement.innerText.split('Catalog No')[1];
                                if (catVal) results.cat_no = catVal.replace(':', '').trim();
                            }
                        }
                        return results;
                    }
                """)
                
                # ì •ê·œì‹ìœ¼ë¡œ SMILES í•œ ë²ˆ ë” ì •ì œ (ì˜ë¬¸/ìˆ«ì/=/#/()/[]/+/- ë“± íŠ¹ìˆ˜ë¬¸ì ì¡°í•©)
                raw_smiles = extracted.get('smiles', '')
                if raw_smiles:
                    match = re.search(r'([A-Za-z0-9#%()=+./@\[\]\\-]+)', raw_smiles)
                    if match: raw_smiles = match.group(1)

                return {
                    "ambeed_cat_no": extracted['cat_no'] or url.split('/')[-1].replace('.html', ''),
                    "cas_number": extracted['cas_no'],
                    "product_name": title.split('|')[0].strip(),
                    "product_url": url,
                    "category": category,
                    "smiles_code": raw_smiles,
                    "body_text": await page.inner_text("body"),
                    "source_name": "Ambeed",
                    "crawled_at": datetime.utcnow().isoformat()
                }
            except Exception as e:
                logger.error(f"Failed to process {url}: {e}")
                return None
            finally: await page.close()

    async def _enrich_with_gemini(self, raw_data: Dict, smiles: Optional[str] = None) -> Dict:
        """Geminië¥¼ ì´ìš©í•œ 3ë‹¨ SMILES ë¶„ë¦¬ ë° ë°ì´í„° ì •ì œ"""
        prompt = f"""
        Extract detailed ADC reagent information from the following content:
        Title: {raw_data['product_name']}
        Body: {raw_data['body_text'][:2000]}
        Input SMILES: {smiles or 'None'}

        Task:
        1. Identify the 'payload_smiles' (the cytotoxic drug part).
        2. Identify the 'linker_smiles' (the chemical linker part).
        3. Identify the 'full_smiles' (the entire drug-linker structure, excluding the antibody).
        4. Extract 'target' and 'summary'.

        Output JSON format:
        {{
            "payload_smiles": "...",
            "linker_smiles": "...",
            "full_smiles": "...",
            "target": "...",
            "summary": "...",
            "properties": {{ "ic50": "...", "solubility": "..." }}
        }}
        If a part is missing, return null. Ensure SMILES strings are valid and complete.
        """
        try:
            model = self._get_model()
            response = await model.generate_content_async(prompt, generation_config=genai.GenerationConfig(response_mime_type="application/json"))
            return json.loads(response.text)
        except Exception as e:
            logger.error(f"Gemini API Error: {e}")
            return {}

    async def run(self, search_term: str, limit: int, job_id: str, start_page: int = 1, batch_size: int = 2):
        from app.api.scheduler import update_job_status
        await update_job_status(job_id, status="running")
        targets = {cat: url for cat, url in self.CATEGORIES.items() if not search_term or search_term == 'all' or search_term.lower() in cat.lower()}
        total = 0
        for cat, url in targets.items():
            total += await self.crawl_category(cat, url, limit, job_id, start_page, batch_size)
        await update_job_status(job_id, status="completed", records_drafted=total, completed_at=datetime.utcnow().isoformat())

ambeed_crawler = AmbeedCrawler()