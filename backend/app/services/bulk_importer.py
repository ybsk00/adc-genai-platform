"""
ClinicalTrials.gov API v2 Importer
ë¤í”„ íŒŒì¼ ëŒ€ì‹  ê³µì‹ API v2ë¥¼ ì‚¬ìš©í•˜ì—¬ ADC ì„ìƒì‹œí—˜ ë°ì´í„° ìˆ˜ì§‘
"""
import aiohttp
import asyncio
import logging
import json
from typing import Dict, Any, List, Optional
from datetime import datetime

from app.core.supabase import supabase

logger = logging.getLogger(__name__)

# ClinicalTrials.gov API v2 ì„¤ì •
API_BASE_URL = "https://clinicaltrials.gov/api/v2/studies"

# ADC ê´€ë ¨ ê²€ìƒ‰ì–´ (Broad Search Mode - í™•ì¥ëœ ì¿¼ë¦¬)
# ì•½ì–´, í•˜ì´í”ˆ ë³€í˜•, ì£¼ìš” ìŠ¹ì¸/ê°œë°œ ì•½ë¬¼ ì´ë¦„ í¬í•¨
ADC_SEARCH_TERMS = [
    # ê¸°ë³¸ ìš©ì–´ + ì£¼ìš” ADC ì•½ë¬¼ ì´ë¦„
    'Antibody Drug Conjugate OR "Antibody-Drug Conjugate" OR ADC OR Immunoconjugate OR '
    'Trastuzumab Deruxtecan OR Enhertu OR DS-8201 OR '
    'Sacituzumab Govitecan OR Trodelvy OR '
    'Brentuximab Vedotin OR Adcetris OR '
    'Ado-trastuzumab Emtansine OR Kadcyla OR T-DM1 OR '
    'Polatuzumab Vedotin OR Polivy OR '
    'Loncastuximab Tesirine OR Zynlonta OR '
    'Tisotumab Vedotin OR Tivdak OR '
    'Mirvetuximab Soravtansine OR Elahere OR '
    'Datopotamab Deruxtecan OR Dato-DXd'
]

# íƒ€ê²Ÿ í‚¤ì›Œë“œ (drug name ì¶”ì¶œìš©)
TARGET_KEYWORDS = ['adc', 'conjugate', 'antibody-drug', 'her2', 'trop2', 'bcma', 'dll3', 'nectin']


class BulkImporter:
    def __init__(self):
        self.total_imported = 0
        self.duplicates_skipped = 0
        self.errors = []

    def extract_study_info(self, study: dict) -> Dict[str, Any]:
        """ì„ìƒì‹œí—˜ ë°ì´í„°ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ"""
        protocol = study.get("protocolSection", {})
        id_module = protocol.get("identificationModule", {})
        status_module = protocol.get("statusModule", {})
        description_module = protocol.get("descriptionModule", {})
        arms_module = protocol.get("armsInterventionsModule", {})
        
        nct_id = id_module.get("nctId", "")
        title = id_module.get("officialTitle") or id_module.get("briefTitle", "No Title")
        
        # ì•½ë¬¼ ì •ë³´ ì¶”ì¶œ (DRUG + BIOLOGICAL íƒ€ì… í¬í•¨ - ADCëŠ” ì¢…ì¢… Biologicalë¡œ ë“±ë¡ë¨)
        interventions = arms_module.get("interventions", [])
        drug_names = [i.get("name", "") for i in interventions if i.get("type") in ["DRUG", "BIOLOGICAL"]]
        
        return {
            "name": title[:200] if title else "Unknown",
            "category": "clinical_trial",
            "description": title,
            "properties": {
                "nct_id": nct_id,
                "phase": status_module.get("phases", []),
                "overall_status": status_module.get("overallStatus"),
                "why_stopped": status_module.get("whyStopped"),
                "brief_summary": description_module.get("briefSummary"),
                "drug_names": drug_names,
                "start_date": status_module.get("startDateStruct", {}).get("date"),
                "completion_date": status_module.get("completionDateStruct", {}).get("date"),
            },
            "status": "draft",
            "outcome_type": self._determine_outcome(status_module),
            "ai_refined": False,
            "enrichment_source": "clinical_trials_api_v2"
        }

    def _determine_outcome(self, status_module: dict) -> str:
        """ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ outcome_type ê²°ì •"""
        status = status_module.get("overallStatus", "").upper()
        why_stopped = status_module.get("whyStopped", "")
        
        if status == "COMPLETED":
            return "Success"
        elif status == "TERMINATED":
            if any(kw in why_stopped.lower() for kw in ["lack of efficacy", "futility", "not effective"]):
                return "Failure"
            return "Terminated"
        elif status in ["WITHDRAWN", "SUSPENDED"]:
            return "Terminated"
        elif status in ["RECRUITING", "ACTIVE_NOT_RECRUITING", "ENROLLING_BY_INVITATION"]:
            return None 
        return None

    async def save_batch(self, batch: List[Dict[str, Any]], job_id: Optional[str] = None):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ DBì— ì €ì¥ (Upsert ì „ëµ: Select -> Insert or Update)"""
        if not batch:
            return 0
        
        saved_count = 0
        for entry in batch:
            try:
                nct_id = entry.get("properties", {}).get("nct_id")
                if not nct_id:
                    continue
                
                # 1. ì¤‘ë³µ ì²´í¬ (nct_id ê¸°ì¤€)
                existing = supabase.table("golden_set_library")\
                    .select("id")\
                    .eq("properties->>nct_id", nct_id)\
                    .execute()
                
                if existing.data:
                    # 2. ì¡´ì¬í•˜ë©´ ì—…ë°ì´íŠ¸ (Upsert íš¨ê³¼)
                    record_id = existing.data[0]['id']
                    # ì—…ë°ì´íŠ¸í•  í•„ë“œë§Œ ì„ íƒ (ê¸°ì¡´ AI ë¶„ì„ ê²°ê³¼ ë“±ì€ ë³´ì¡´í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆìŒ)
                    # ì—¬ê¸°ì„œëŠ” ìµœì‹  ë°ì´í„°ë¡œ ë®ì–´ì“°ê¸° (properties ë“±)
                    update_data = {
                        "name": entry["name"],
                        "description": entry["description"],
                        "properties": entry["properties"],
                        "outcome_type": entry["outcome_type"],
                        # statusë‚˜ ai_refinedëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ (ì´ë¯¸ ì‘ì—… ì¤‘ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
                    }
                    supabase.table("golden_set_library").update(update_data).eq("id", record_id).execute()
                    # logger.info(f"Updated existing record: {nct_id}")
                else:
                    # 3. ì—†ìœ¼ë©´ ì‚½ì…
                    supabase.table("golden_set_library").insert(entry).execute()
                    saved_count += 1
                    self.total_imported += 1
                
            except Exception as e:
                logger.error(f"Save/Update error for {nct_id}: {e}")
                self.errors.append(str(e))
        
        return saved_count

    async def fetch_studies(self, search_term: str, status_filter: List[str], page_size: int = 100, max_pages: int = 100, mode: str = "daily") -> List[dict]:
        """API v2ë¡œ ì„ìƒì‹œí—˜ ë°ì´í„° ì¡°íšŒ (í˜ì´ì§€ë„¤ì´ì…˜ & ëª¨ë“œ ì§€ì›)"""
        all_studies = []
        next_page_token = None
        page = 0
        
        # ë‚ ì§œ í•„í„° ì„¤ì • (Daily Syncìš©)
        last_update_date = None
        if mode == "daily":
            from datetime import timedelta
            yesterday = datetime.utcnow() - timedelta(days=1)
            last_update_date = yesterday.strftime("%Y-%m-%d")
            logger.info(f"ğŸ“… Daily Sync Mode: Fetching updates since {last_update_date}")
        
        async with aiohttp.ClientSession() as session:
            while page < max_pages:
                params = {
                    "query.term": search_term,
                    "filter.overallStatus": status_filter,
                    "pageSize": page_size,
                    "format": "json"
                }
                
                if next_page_token:
                    params["pageToken"] = next_page_token
                
                if last_update_date:
                    params["filter.lastUpdatePostDate"] = last_update_date
                
                try:
                    async with session.get(API_BASE_URL, params=params, timeout=aiohttp.ClientTimeout(total=120)) as response:
                        if response.status != 200:
                            logger.error(f"API Error: HTTP {response.status}")
                            break
                        
                        data = await response.json()
                        studies = data.get("studies", [])
                        
                        if not studies:
                            break
                        
                        all_studies.extend(studies)
                        logger.info(f"ğŸ“¥ Fetched {len(studies)} studies (page {page + 1}, term: {search_term[:30]}...)")
                        
                        next_page_token = data.get("nextPageToken")
                        if not next_page_token:
                            break
                        
                        page += 1
                        await asyncio.sleep(0.5)  # Rate limiting (0.5ì´ˆ íœ´ì‹)
                        
                except Exception as e:
                    logger.error(f"Fetch error: {e}")
                    break
        
        return all_studies

    async def run_import(self, job_id: Optional[str] = None, max_studies: int = 5000, mode: str = "daily"):
        """
        ClinicalTrials.gov API v2ë¥¼ ì‚¬ìš©í•˜ì—¬ ADC ì„ìƒì‹œí—˜ ë°ì´í„° ìˆ˜ì§‘
        mode: 'daily' (ê¸°ë³¸ê°’, ì–´ì œ ì´í›„ ë³€ê²½ë¶„) ë˜ëŠ” 'full' (ì „ì²´ ë¤í”„)
        """
        from app.api.scheduler import update_job_status, is_cancelled
        
        if job_id:
            await update_job_status(job_id, status="running")
        
        logger.info(f"ğŸš€ [API v2 Importer] Starting ClinicalTrials.gov Broad Search (Mode: {mode})...")
        
        batch = []
        batch_size = 50
        
        try:
            # ìƒíƒœ í•„í„°: ëª¨ë“  ìƒíƒœ í¬í•¨ (Broad Search)
            status_filter = [
                "COMPLETED", "TERMINATED", "WITHDRAWN", 
                "SUSPENDED", "RECRUITING", "ACTIVE_NOT_RECRUITING",
                "ENROLLING_BY_INVITATION", "NOT_YET_RECRUITING", "UNKNOWN"
            ]
            
            total_fetched = 0
            
            # Full ëª¨ë“œì¼ ë•ŒëŠ” í˜ì´ì§€ ì œí•œì„ ë„‰ë„‰í•˜ê²Œ
            # Daily ëª¨ë“œì¼ ë•ŒëŠ” ì ê²Œ (ì–´ì°¨í”¼ ë‚ ì§œ í•„í„°ë¡œ ê±¸ëŸ¬ì§)
            max_pages_per_term = 100 if mode == "full" else 10
            
            for search_term in ADC_SEARCH_TERMS:
                if self.total_imported >= max_studies:
                    logger.info(f"âœ… Reached max studies limit: {max_studies}")
                    break
                
                # ì¤‘ë‹¨ ìš”ì²­ ì²´í¬
                if job_id and await is_cancelled(job_id):
                    logger.info("Import cancelled by user")
                    await update_job_status(job_id, status="stopped")
                    return
                
                # ë‹¨ì¼ í†µí•© ì¿¼ë¦¬ë¡œ ëª¨ë“  ìƒíƒœ ì¡°íšŒ
                studies = await self.fetch_studies(
                    search_term=search_term,
                    status_filter=status_filter,
                    page_size=100,
                    max_pages=max_pages_per_term,
                    mode=mode
                )
                
                for study in studies:
                    if self.total_imported >= max_studies:
                        break
                    
                    entry = self.extract_study_info(study)
                    batch.append(entry)
                    total_fetched += 1
                    
                    # ë°°ì¹˜ ì €ì¥
                    if len(batch) >= batch_size:
                        saved = await self.save_batch(batch, job_id)
                        logger.info(f"ğŸ’¾ Batch saved: {saved} new records")
                        batch = []
                        
                        if job_id:
                            await update_job_status(
                                job_id, 
                                records_found=total_fetched,
                                records_drafted=self.total_imported
                            )
            
            # ë‚¨ì€ ë°°ì¹˜ ì €ì¥
            if batch:
                await self.save_batch(batch, job_id)
            
            logger.info(f"""
            âœ… [API v2 Import Complete]
            - Mode: {mode}
            - Total Fetched: {total_fetched}
            - New Records: {self.total_imported}
            - Duplicates Skipped: {self.duplicates_skipped}
            - Errors: {len(self.errors)}
            """)
            
            if job_id:
                # ì¤‘ë³µ ìŠ¤í‚µ ì •ë³´ë¥¼ errors í•„ë“œì— ì •ë³´ì„±ìœ¼ë¡œ ì¶”ê°€ (UI í‘œì‹œìš©)
                completion_info = []
                if self.duplicates_skipped > 0:
                    completion_info.append(f"Info: {self.duplicates_skipped} records skipped (duplicate).")
                
                await update_job_status(
                    job_id,
                    status="completed",
                    records_found=total_fetched,
                    records_drafted=self.total_imported,
                    completed_at=datetime.utcnow().isoformat(),
                    errors=completion_info + self.errors # ê¸°ì¡´ ì—ëŸ¬ì— ì •ë³´ ì¶”ê°€
                )
                
        except Exception as e:
            error_msg = f"Import failed: {str(e)}"
            logger.error(error_msg)
            self.errors.append(error_msg)
            
            if job_id:
                await update_job_status(
                    job_id,
                    status="failed",
                    errors=self.errors
                )


bulk_importer = BulkImporter()
