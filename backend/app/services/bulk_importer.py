"""
ClinicalTrials.gov Bulk Importer Service
API ì°¨ë‹¨ ìš°íšŒë¥¼ ìœ„í•œ JSON ë¤í”„ ë‹¤ìš´ë¡œë“œ ë° ì¼ê´„ ì ì¬
"""
import asyncio
import aiohttp
import zipfile
import io
import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.core.supabase import supabase

logger = logging.getLogger(__name__)

# ClinicalTrials.gov ì „ì²´ ë°ì´í„° ë¤í”„ URL
DUMP_URL = "https://clinicaltrials.gov/AllPublicJSON.zip"

# ADC ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°
TARGET_KEYWORDS = [
    "antibody drug conjugate", "antibody-drug conjugate", "adc",
    "her2", "trop2", "egfr", "cd19", "cd22", "cd33", "cd30",
    "nectin-4", "bcma", "folate receptor",
    "vedotin", "deruxtecan", "govitecan", "emtansine", "ozogamicin",
    "mafodotin", "trastuzumab", "sacituzumab", "enfortumab",
    "polatuzumab", "brentuximab", "inotuzumab", "gemtuzumab"
]

class BulkImporter:
    def __init__(self):
        self.total_processed = 0
        self.total_imported = 0
        self.errors = []

    def is_adc_related(self, study_data: dict) -> bool:
        """ADC ê´€ë ¨ ì„ìƒì‹œí—˜ì¸ì§€ í•„í„°ë§"""
        try:
            # ì „ì²´ JSONì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í‚¤ì›Œë“œ ê²€ìƒ‰
            full_text = json.dumps(study_data).lower()
            return any(keyword in full_text for keyword in TARGET_KEYWORDS)
        except Exception:
            return False

    def extract_study_info(self, study_data: dict) -> Dict[str, Any]:
        """ì„ìƒì‹œí—˜ ë°ì´í„°ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ"""
        protocol = study_data.get("protocolSection", {})
        id_module = protocol.get("identificationModule", {})
        status_module = protocol.get("statusModule", {})
        description_module = protocol.get("descriptionModule", {})
        
        nct_id = id_module.get("nctId", "")
        title = id_module.get("officialTitle") or id_module.get("briefTitle", "No Title")
        
        return {
            "name": title[:200] if title else "Unknown",
            "category": "clinical_trial",
            "description": title,
            "properties": {
                "nct_id": nct_id,
                "phase": status_module.get("phase"),
                "overall_status": status_module.get("overallStatus"),
                "why_stopped": status_module.get("whyStopped"),
                "brief_summary": description_module.get("briefSummary"),
                "raw_data": study_data  # ì „ì²´ ì›ë³¸ ë°ì´í„° ì €ì¥ (AI Refinerìš©)
            },
            "status": "draft",
            "outcome_type": "Unknown",  # AI Refinerê°€ ë‚˜ì¤‘ì— ì±„ì›€
            "ai_refined": False,  # ë¯¸ì •ì œ ìƒíƒœ
            "enrichment_source": "clinical_trials_bulk"
        }

    async def save_batch(self, batch: List[Dict[str, Any]], job_id: Optional[str] = None):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ DBì— ì €ì¥ (upsert)"""
        if not batch:
            return 0
        
        saved_count = 0
        for entry in batch:
            try:
                nct_id = entry.get("properties", {}).get("nct_id")
                if not nct_id:
                    continue
                
                # ì¤‘ë³µ ì²´í¬ (nct_id ê¸°ì¤€)
                existing = supabase.table("golden_set_library")\
                    .select("id")\
                    .eq("properties->>nct_id", nct_id)\
                    .execute()
                
                if not existing.data:
                    supabase.table("golden_set_library").insert(entry).execute()
                    saved_count += 1
            except Exception as e:
                self.errors.append(f"Save error for {entry.get('properties', {}).get('nct_id')}: {str(e)[:100]}")
        
        return saved_count

    async def run_import(self, job_id: Optional[str] = None, max_studies: int = 5000):
        """
        Bulk Import ì‹¤í–‰
        - JSON ë¤í”„ ë‹¤ìš´ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë°)
        - ADC ê´€ë ¨ ë°ì´í„° í•„í„°ë§
        - ì¼ê´„ DB ì ì¬
        """
        from app.api.scheduler import update_job_status, is_cancelled
        
        if job_id:
            await update_job_status(job_id, status="running")
        
        logger.info("ğŸš€ [Bulk Importer] Starting ClinicalTrials.gov dump download...")
        
        batch = []
        batch_size = 100
        
        try:
            # ClinicalTrials.gov 403 ë°©ì§€ë¥¼ ìœ„í•œ ì™„ì „í•œ ë¸Œë¼ìš°ì € í—¤ë”
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/zip,application/octet-stream,*/*',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Referer': 'https://clinicaltrials.gov/',
            }
            
            async with aiohttp.ClientSession(headers=headers) as session:
                logger.info(f"ğŸ“¥ Downloading from {DUMP_URL}...")
                
                async with session.get(DUMP_URL, timeout=aiohttp.ClientTimeout(total=3600)) as response:
                    if response.status != 200:
                        error_msg = f"Download failed: HTTP {response.status}"
                        logger.error(error_msg)
                        if job_id:
                            await update_job_status(job_id, status="failed", errors=[error_msg])
                        return
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ë‹¤ìš´ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì : 2GB â†’ ~10MB)
                    import tempfile
                    import os
                    temp_path = None
                    
                    try:
                        logger.info("ğŸ“¦ Streaming ZIP file to temp... (this may take several minutes)")
                        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.zip')
                        temp_path = temp_file.name
                        downloaded_size = 0
                        
                        async for chunk in response.content.iter_chunked(1024 * 1024):  # 1MB chunks
                            temp_file.write(chunk)
                            downloaded_size += len(chunk)
                            if downloaded_size % (100 * 1024 * 1024) == 0:  # Log every 100MB
                                logger.info(f"ğŸ“¥ Downloaded {downloaded_size // (1024*1024)} MB...")
                        
                        temp_file.close()
                        logger.info(f"âœ… Download complete: {downloaded_size // (1024*1024)} MB")
                        
                        if job_id:
                            await update_job_status(job_id, records_found=0)
                        
                        logger.info("ğŸ“‚ Extracting and parsing JSON files...")
                        
                        with zipfile.ZipFile(temp_path) as z:
                            json_files = [f for f in z.namelist() if f.endswith('.json')]
                            total_files = len(json_files)
                            logger.info(f"Found {total_files} JSON files in archive")
                        
                            for idx, filename in enumerate(json_files):
                                # ì¤‘ë‹¨ ìš”ì²­ ì²´í¬
                                if job_id and await is_cancelled(job_id):
                                    logger.info("Import cancelled by user")
                                    await update_job_status(job_id, status="stopped")
                                    return
                                
                                # ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ ì²´í¬
                                if self.total_imported >= max_studies:
                                    logger.info(f"Reached max studies limit: {max_studies}")
                                    break
                                
                                try:
                                    with z.open(filename) as f:
                                        study_data = json.load(f)
                                        self.total_processed += 1
                                        
                                        # ADC ê´€ë ¨ ë°ì´í„°ë§Œ í•„í„°ë§
                                        if self.is_adc_related(study_data):
                                            entry = self.extract_study_info(study_data)
                                            batch.append(entry)
                                            
                                            # ë°°ì¹˜ê°€ ì°¨ë©´ ì €ì¥
                                            if len(batch) >= batch_size:
                                                saved = await self.save_batch(batch, job_id)
                                                self.total_imported += saved
                                                batch = []
                                                
                                                if job_id:
                                                    await update_job_status(
                                                        job_id, 
                                                        records_found=self.total_processed,
                                                        records_drafted=self.total_imported
                                                    )
                                                
                                                logger.info(f"Progress: {self.total_processed}/{total_files} files, {self.total_imported} ADC studies imported")
                                
                                except json.JSONDecodeError:
                                    continue
                                except Exception as e:
                                    self.errors.append(str(e)[:100])
                        
                            # ë‚¨ì€ ë°°ì¹˜ ì €ì¥
                            if batch:
                                saved = await self.save_batch(batch, job_id)
                                self.total_imported += saved
                    
                    finally:
                        # ì„ì‹œ íŒŒì¼ ë°˜ë“œì‹œ ì‚­ì œ (ë””ìŠ¤í¬ ê³µê°„ í™•ë³´)
                        if temp_path and os.path.exists(temp_path):
                            os.unlink(temp_path)
                            logger.info(f"ğŸ—‘ï¸ Temp file deleted: {temp_path}")
            
            # ì™„ë£Œ
            logger.info(f"ğŸ‰ Import Complete! Total: {self.total_imported} ADC studies from {self.total_processed} files")
            
            if job_id:
                await update_job_status(
                    job_id,
                    status="completed",
                    records_found=self.total_processed,
                    records_drafted=self.total_imported,
                    completed_at=datetime.utcnow().isoformat(),
                    errors=self.errors[:20]
                )
        
        except Exception as e:
            logger.error(f"Bulk Import Error: {e}")
            if job_id:
                await update_job_status(job_id, status="failed", errors=[str(e)])

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
bulk_importer = BulkImporter()
