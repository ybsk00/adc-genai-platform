"""
ClinicalTrials.gov API v2 Importer
ë¤í”„ íŒŒì¼ ëŒ€ì‹  ê³µì‹ API v2ë¥¼ ì‚¬ìš©í•˜ì—¬ ADC ì„ìƒì‹œí—˜ ë°ì´í„° ìˆ˜ì§‘
"""
import aiohttp
import asyncio
import logging
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
import os

from app.core.supabase import supabase

logger = logging.getLogger(__name__)

# ClinicalTrials.gov API v2 ì„¤ì •
API_BASE_URL = "https://clinicaltrials.gov/api/v2/studies"

# ADC ê´€ë ¨ ê²€ìƒ‰ì–´ (Split Mode - ì•ˆì •ì„± ë° ì§„í–‰ë¥  í‘œì‹œ ìµœì í™”)
ADC_SEARCH_TERMS = [
    'Antibody Drug Conjugate',
    '"Antibody-Drug Conjugate"',
    'ADC AND (Cancer OR Tumor OR Oncology OR Neoplasm)', # ë…¸ì´ì¦ˆ ë°©ì§€ìš© ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    'Immunoconjugate',
    'Trastuzumab Deruxtecan',
    'Enhertu',
    'DS-8201',
    'Sacituzumab Govitecan',
    'Trodelvy',
    'Brentuximab Vedotin',
    'Adcetris',
    'Ado-trastuzumab Emtansine',
    'Kadcyla',
    'T-DM1',
    'Polatuzumab Vedotin',
    'Polivy',
    'Loncastuximab Tesirine',
    'Zynlonta',
    'Tisotumab Vedotin',
    'Tivdak',
    'Mirvetuximab Soravtansine',
    'Elahere',
    'Datopotamab Deruxtecan',
    'Dato-DXd'
]

# íƒ€ê²Ÿ í‚¤ì›Œë“œ (drug name ì¶”ì¶œìš©)
TARGET_KEYWORDS = ['adc', 'conjugate', 'antibody-drug', 'her2', 'trop2', 'bcma', 'dll3', 'nectin']


class BulkImporter:
    def __init__(self):
        self.total_imported = 0
        self.duplicates_skipped = 0
        self.errors = []

    def extract_study_info(self, study: dict) -> Dict[str, Any]:
        """ì„ìƒì‹œí—˜ ë°ì´í„°ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ (ResultsSection í¬í•¨)"""
        protocol = study.get("protocolSection", {})
        results = study.get("resultsSection", {})
        
        id_module = protocol.get("identificationModule", {})
        status_module = protocol.get("statusModule", {})
        description_module = protocol.get("descriptionModule", {})
        arms_module = protocol.get("armsInterventionsModule", {})
        
        nct_id = id_module.get("nctId", "")
        title = id_module.get("officialTitle") or id_module.get("briefTitle", "No Title")
        
        # ì•½ë¬¼ ì •ë³´ ì¶”ì¶œ
        interventions = arms_module.get("interventions", [])
        drug_names = [i.get("name", "") for i in interventions if i.get("type") in ["DRUG", "BIOLOGICAL"]]
        
        # --- ì •ëŸ‰ì  ë°ì´í„° ì¶”ì¶œ (ResultsSection) ---
        outcome_data = self._parse_results_section(results)
        
        # ê¸°ë³¸ ì •ë³´ êµ¬ì„±
        info = {
            "name": title[:200] if title else "Unknown",
            "category": "clinical_trial",
            "description": title,
            "properties": {
                "nct_id": nct_id,
                "phase": status_module.get("phases", []),
                "overall_status": status_module.get("overallStatus"),
                "why_stopped": status_module.get("whyStopped"),
                "brief_summary": description_module.get("briefSummary"),
                "drug_names": drug_names,
                "start_date": status_module.get("startDateStruct", {}).get("date"),
                "completion_date": status_module.get("completionDateStruct", {}).get("date"),
            },
            "status": "draft",
            "outcome_type": self._determine_outcome(status_module),
            "ai_refined": False,
            "enrichment_source": "clinical_trials_api_v2",
            "confidence_score": 0.95 if outcome_data.get("has_results") else 0.5
        }
        
        # ì •ëŸ‰ ë°ì´í„° ë³‘í•© (orr_pct, os_months, pfs_months, patient_count ë“±)
        info.update(outcome_data.get("metrics", {}))
        
        return info

    def _parse_results_section(self, results: dict) -> Dict[str, Any]:
        """ResultsSectionì—ì„œ ORR, OS, PFS, Patient Count ë“± ì¶”ì¶œ"""
        metrics = {
            "orr_pct": None,
            "os_months": None,
            "pfs_months": None,
            "dor_months": None,
            "patient_count": None,
            "adverse_events_grade3_pct": None
        }
        
        if not results:
            return {"has_results": False, "metrics": metrics}
            
        # 1. í™˜ì ìˆ˜ (Participant Flow)
        participant_flow = results.get("participantFlowModule", {})
        groups = participant_flow.get("groups", [])
        if groups:
            try:
                # 'Total' ê·¸ë£¹ì´ ëª…ì‹œì ìœ¼ë¡œ ìˆëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ í•©ì‚° ë°©ì§€)
                total_group = next((g for g in groups if "total" in g.get("title", "").lower()), None)
                if total_group:
                    metrics["patient_count"] = int(total_group.get("count", 0))
                else:
                    # Arms í•©ì‚°
                    metrics["patient_count"] = sum(int(g.get("count", 0)) for g in groups if g.get("count"))
            except:
                pass

        # 2. íš¨ëŠ¥ ì§€í‘œ (Outcome Measures)
        outcome_measures = results.get("outcomeMeasuresModule", {}).get("outcomeMeasures", [])
        for measure in outcome_measures:
            title = measure.get("title", "").lower()
            unit = measure.get("unitOfMeasure", "").lower()
            classes = measure.get("classes", [])
            if not classes: continue
            
            # ì²« ë²ˆì§¸ í´ë˜ìŠ¤ì˜ ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì‚¬ìš© (ë‹¨ìˆœí™”)
            categories = classes[0].get("categories", [])
            if not categories: continue
            
            measurements = categories[0].get("measurements", [])
            if not measurements: continue
            
            value_str = measurements[0].get("value")
            if not value_str: continue
            
            try:
                value = float(value_str)
                
                # í‚¤ì›Œë“œ ë° ë‹¨ìœ„ ê²€ì¦ (AI 2ì°¨ ê²€ì¦ ë¡œì§)
                if any(kw in title for kw in ["objective response rate", "orr", "overall response rate"]):
                    if self._verify_unit(unit, "percentage"):
                        metrics["orr_pct"] = value
                elif any(kw in title for kw in ["overall survival", "os"]) and "month" in title:
                    if self._verify_unit(unit, "months"):
                        metrics["os_months"] = value
                elif any(kw in title for kw in ["progression-free survival", "pfs"]) and "month" in title:
                    if self._verify_unit(unit, "months"):
                        metrics["pfs_months"] = value
                elif any(kw in title for kw in ["duration of response", "dor"]) and "month" in title:
                    if self._verify_unit(unit, "months"):
                        metrics["dor_months"] = value
            except ValueError:
                continue

        return {"has_results": True, "metrics": metrics}

    def _verify_unit(self, unit_str: str, expected_type: str) -> bool:
        """ë‹¨ìœ„ ê²€ì¦: percentage vs months"""
        if not unit_str:
            return True # ë‹¨ìœ„ê°€ ì—†ìœ¼ë©´ ì¼ë‹¨ í—ˆìš© (ë³´ìˆ˜ì  ì ‘ê·¼)
            
        unit_str = unit_str.lower()
        if expected_type == "percentage":
            # % ë‹¨ìœ„ í™•ì¸ (percent, %, percentage)
            return any(u in unit_str for u in ["%", "percent"])
        elif expected_type == "months":
            # ì‹œê°„ ë‹¨ìœ„ í™•ì¸ (month, week, year, day)
            return any(u in unit_str for u in ["month", "week", "year", "day"])
        return True

    def _determine_outcome(self, status_module: dict) -> str:
        """ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ outcome_type ê²°ì •"""
        status = status_module.get("overallStatus", "").upper()
        why_stopped = status_module.get("whyStopped", "")
        
        if status == "COMPLETED":
            return "Success"
        elif status == "TERMINATED":
            if any(kw in why_stopped.lower() for kw in ["lack of efficacy", "futility", "not effective"]):
                return "Failure"
            return "Terminated"
        elif status in ["WITHDRAWN", "SUSPENDED"]:
            return "Terminated"
        elif status in ["RECRUITING", "ACTIVE_NOT_RECRUITING", "ENROLLING_BY_INVITATION"]:
            return None 
        return None

    async def save_batch(self, batch: List[Dict[str, Any]], job_id: Optional[str] = None):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ DBì— ì €ì¥ (Upsert ì „ëµ: Select -> Insert or Update)"""
        if not batch:
            return 0
        
        saved_count = 0
        for entry in batch:
            try:
                nct_id = entry.get("properties", {}).get("nct_id")
                if not nct_id:
                    continue
                
                # 1. ì¤‘ë³µ ì²´í¬ (nct_id ê¸°ì¤€)
                existing = supabase.table("golden_set_library")\
                    .select("id")\
                    .eq("properties->>nct_id", nct_id)\
                    .execute()
                
                if existing.data:
                    # 2. ì¡´ì¬í•˜ë©´ ì—…ë°ì´íŠ¸ (Upsert íš¨ê³¼)
                    record_id = existing.data[0]['id']
                    # ì—…ë°ì´íŠ¸í•  í•„ë“œë§Œ ì„ íƒ (ê¸°ì¡´ AI ë¶„ì„ ê²°ê³¼ ë“±ì€ ë³´ì¡´í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆìŒ)
                    # ì—¬ê¸°ì„œëŠ” ìµœì‹  ë°ì´í„°ë¡œ ë®ì–´ì“°ê¸° (properties ë“±)
                    update_data = {
                        "name": entry["name"],
                        "description": entry["description"],
                        "properties": entry["properties"],
                        "outcome_type": entry["outcome_type"],
                        # statusë‚˜ ai_refinedëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ (ì´ë¯¸ ì‘ì—… ì¤‘ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
                    }
                    supabase.table("golden_set_library").update(update_data).eq("id", record_id).execute()
                    # logger.info(f"Updated existing record: {nct_id}")
                else:
                    # 3. ì—†ìœ¼ë©´ ì‚½ì…
                    supabase.table("golden_set_library").insert(entry).execute()
                    saved_count += 1
                    self.total_imported += 1
                
            except Exception as e:
                logger.error(f"Save/Update error for {nct_id}: {e}")
                self.errors.append(str(e))
        
        return saved_count

    async def fetch_studies_generator(self, search_term: str, status_filter: List[str], page_size: int = 100, max_pages: int = 100, mode: str = "daily"):
        """API v2ë¡œ ì„ìƒì‹œí—˜ ë°ì´í„° ì¡°íšŒ (ì œë„ˆë ˆì´í„° ë°©ì‹ - ì¦ë¶„ ì²˜ë¦¬ ì§€ì›)"""
        next_page_token = None
        page = 0
        
        # ë‚ ì§œ í•„í„° ì„¤ì • (Daily Syncìš©)
        last_update_date = None
        if mode == "daily":
            from datetime import timedelta
            yesterday = datetime.utcnow() - timedelta(days=1)
            last_update_date = yesterday.strftime("%Y-%m-%d")
            logger.info(f"ğŸ“… Daily Sync Mode: Fetching updates since {last_update_date}")
        
        # í”„ë¡ì‹œ ì„¤ì •
        proxy_url = None
        if os.getenv("PROXY_ENABLED", "").lower() == "true":
            host = os.getenv("PROXY_HOST")
            port = os.getenv("PROXY_PORT")
            user = os.getenv("PROXY_USERNAME")
            password = os.getenv("PROXY_PASSWORD")
            
            if host and port:
                if user and password:
                    proxy_url = f"http://{user}:{password}@{host}:{port}"
                else:
                    proxy_url = f"http://{host}:{port}"
                logger.info(f"ğŸŒ Using proxy: {host}:{port}")

        async with aiohttp.ClientSession() as session:
            while page < max_pages:
                # API v2 syntax for date filtering
                query_term = search_term
                if last_update_date:
                    query_term = f"{search_term} AND AREA[LastUpdatePostDate]RANGE[{last_update_date},MAX]"
                
                params = {
                    "query.term": query_term,
                    "filter.overallStatus": ",".join(status_filter) if isinstance(status_filter, list) else status_filter,
                    "pageSize": page_size,
                    "format": "json"
                }
                
                if next_page_token:
                    params["pageToken"] = next_page_token
                
                # if last_update_date:
                #     params["filter.lastUpdatePostDate"] = last_update_date
                
                try:
                    request_kwargs = {
                        "params": params,
                        "timeout": aiohttp.ClientTimeout(total=120)
                    }
                    if proxy_url:
                        request_kwargs["proxy"] = proxy_url
                        
                    async with session.get(API_BASE_URL, **request_kwargs) as response:
                        if response.status != 200:
                            logger.error(f"API Error: HTTP {response.status}")
                            break
                        
                        data = await response.json()
                        studies = data.get("studies", [])
                        
                        if not studies:
                            break
                        
                        yield studies
                        
                        next_page_token = data.get("nextPageToken")
                        if not next_page_token:
                            break
                        
                        page += 1
                        await asyncio.sleep(0.5)  # Rate limiting
                        
                except Exception as e:
                    logger.error(f"Fetch error: {e}")
                    break

    async def run_import(self, job_id: Optional[str] = None, max_studies: int = 5000, mode: str = "daily"):
        """
        ClinicalTrials.gov API v2ë¥¼ ì‚¬ìš©í•˜ì—¬ ADC ì„ìƒì‹œí—˜ ë°ì´í„° ìˆ˜ì§‘
        mode: 'daily' (ê¸°ë³¸ê°’, ì–´ì œ ì´í›„ ë³€ê²½ë¶„) ë˜ëŠ” 'full' (ì „ì²´ ë¤í”„)
        """
        from app.api.scheduler import update_job_status, is_cancelled
        
        if job_id:
            await update_job_status(job_id, status="running")
        
        logger.info(f"ğŸš€ [API v2 Importer] Starting ClinicalTrials.gov Split Search (Mode: {mode})...")
        
        try:
            # ìƒíƒœ í•„í„°: ëª¨ë“  ìƒíƒœ í¬í•¨ (Broad Search)
            status_filter = [
                "COMPLETED", "TERMINATED", "WITHDRAWN", 
                "SUSPENDED", "RECRUITING", "ACTIVE_NOT_RECRUITING",
                "ENROLLING_BY_INVITATION", "NOT_YET_RECRUITING", "UNKNOWN"
            ]
            
            total_fetched = 0
            
            # Full ëª¨ë“œì¼ ë•ŒëŠ” í˜ì´ì§€ ì œí•œì„ ë„‰ë„‰í•˜ê²Œ
            # Daily ëª¨ë“œì¼ ë•ŒëŠ” ì ê²Œ (ì–´ì°¨í”¼ ë‚ ì§œ í•„í„°ë¡œ ê±¸ëŸ¬ì§)
            max_pages_per_term = 100 if mode == "full" else 10
            
            for search_term in ADC_SEARCH_TERMS:
                if self.total_imported >= max_studies:
                    logger.info(f"âœ… Reached max studies limit: {max_studies}")
                    break
                
                # ì¤‘ë‹¨ ìš”ì²­ ì²´í¬
                if job_id and await is_cancelled(job_id):
                    logger.info("Import cancelled by user")
                    await update_job_status(job_id, status="stopped")
                    return
                
                logger.info(f"ğŸ” Searching for: {search_term}")
                
                # í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì¦‰ì‹œ ì²˜ë¦¬
                async for studies_page in self.fetch_studies_generator(
                    search_term=search_term,
                    status_filter=status_filter,
                    page_size=100,
                    max_pages=max_pages_per_term,
                    mode=mode
                ):
                    total_fetched += len(studies_page)
                    
                    batch = []
                    for study in studies_page:
                        if self.total_imported >= max_studies:
                            break
                        
                        entry = self.extract_study_info(study)
                        batch.append(entry)
                    
                    # í˜ì´ì§€ ë‹¨ìœ„ ì¦‰ì‹œ ì €ì¥
                    if batch:
                        saved = await self.save_batch(batch, job_id)
                        logger.info(f"ğŸ’¾ Page saved: {saved} new records (Term: {search_term[:20]}...)")
                        
                        # ì§„í–‰ë¥  ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
                        if job_id:
                            await update_job_status(
                                job_id, 
                                records_found=total_fetched,
                                records_drafted=self.total_imported
                            )
                    
                    if self.total_imported >= max_studies:
                        break
            
            logger.info(f"""
            âœ… [API v2 Import Complete]
            - Mode: {mode}
            - Total Fetched: {total_fetched}
            - New Records: {self.total_imported}
            - Duplicates Skipped: {self.duplicates_skipped}
            - Errors: {len(self.errors)}
            """)
            
            if job_id:
                # ì¤‘ë³µ ìŠ¤í‚µ ì •ë³´ë¥¼ errors í•„ë“œì— ì •ë³´ì„±ìœ¼ë¡œ ì¶”ê°€ (UI í‘œì‹œìš©)
                completion_info = []
                if self.duplicates_skipped > 0:
                    completion_info.append(f"Info: {self.duplicates_skipped} records skipped (duplicate).")
                
                await update_job_status(
                    job_id,
                    status="completed",
                    records_found=total_fetched,
                    records_drafted=self.total_imported,
                    completed_at=datetime.utcnow().isoformat(),
                    errors=completion_info + self.errors # ê¸°ì¡´ ì—ëŸ¬ì— ì •ë³´ ì¶”ê°€
                )
                
        except Exception as e:
            error_msg = f"Import failed: {str(e)}"
            logger.error(error_msg)
            self.errors.append(error_msg)
            
            if job_id:
                await update_job_status(
                    job_id,
                    status="failed",
                    errors=self.errors
                )


bulk_importer = BulkImporter()
