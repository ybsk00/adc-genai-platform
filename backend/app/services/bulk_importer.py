"""
ClinicalTrials.gov Bulk Importer Service
API Ï∞®Îã® Ïö∞ÌöåÎ•º ÏúÑÌïú JSON Îç§ÌîÑ Îã§Ïö¥Î°úÎìú Î∞è ÏùºÍ¥Ñ Ï†ÅÏû¨
"""
import asyncio
import aiohttp
import zipfile
import io
import json
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.core.supabase import supabase

logger = logging.getLogger(__name__)

# ClinicalTrials.gov Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ Îç§ÌîÑ URL
DUMP_URL = "https://clinicaltrials.gov/AllPublicJSON.zip"

# ADC Í¥ÄÎ†® ÌÇ§ÏõåÎìú ÌïÑÌÑ∞
TARGET_KEYWORDS = [
    "antibody drug conjugate", "antibody-drug conjugate", "adc",
    "her2", "trop2", "egfr", "cd19", "cd22", "cd33", "cd30",
    "nectin-4", "bcma", "folate receptor",
    "vedotin", "deruxtecan", "govitecan", "emtansine", "ozogamicin",
    "mafodotin", "trastuzumab", "sacituzumab", "enfortumab",
    "polatuzumab", "brentuximab", "inotuzumab", "gemtuzumab"
]

class BulkImporter:
    def __init__(self):
        self.total_processed = 0
        self.total_imported = 0
        self.errors = []

    def is_adc_related(self, study_data: dict) -> bool:
        """ADC Í¥ÄÎ†® ÏûÑÏÉÅÏãúÌóòÏù∏ÏßÄ ÌïÑÌÑ∞ÎßÅ"""
        try:
            # Ï†ÑÏ≤¥ JSONÏùÑ Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôòÌïòÏó¨ ÌÇ§ÏõåÎìú Í≤ÄÏÉâ
            full_text = json.dumps(study_data).lower()
            return any(keyword in full_text for keyword in TARGET_KEYWORDS)
        except Exception:
            return False

    def extract_study_info(self, study_data: dict) -> Dict[str, Any]:
        """ÏûÑÏÉÅÏãúÌóò Îç∞Ïù¥ÌÑ∞ÏóêÏÑú ÌïÑÏöîÌïú Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        protocol = study_data.get("protocolSection", {})
        id_module = protocol.get("identificationModule", {})
        status_module = protocol.get("statusModule", {})
        description_module = protocol.get("descriptionModule", {})
        
        nct_id = id_module.get("nctId", "")
        title = id_module.get("officialTitle") or id_module.get("briefTitle", "No Title")
        
        return {
            "name": title[:200] if title else "Unknown",
            "category": "clinical_trial",
            "description": title,
            "properties": {
                "nct_id": nct_id,
                "phase": status_module.get("phase"),
                "overall_status": status_module.get("overallStatus"),
                "why_stopped": status_module.get("whyStopped"),
                "brief_summary": description_module.get("briefSummary"),
                "raw_data": study_data  # Ï†ÑÏ≤¥ ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• (AI RefinerÏö©)
            },
            "status": "draft",
            "outcome_type": "Unknown",  # AI RefinerÍ∞Ä ÎÇòÏ§ëÏóê Ï±ÑÏõÄ
            "ai_refined": False,  # ÎØ∏Ï†ïÏ†ú ÏÉÅÌÉú
            "enrichment_source": "clinical_trials_bulk"
        }

    async def save_batch(self, batch: List[Dict[str, Any]], job_id: Optional[str] = None):
        """Î∞∞Ïπò Îã®ÏúÑÎ°ú DBÏóê Ï†ÄÏû• (upsert)"""
        if not batch:
            return 0
        
        saved_count = 0
        for entry in batch:
            try:
                nct_id = entry.get("properties", {}).get("nct_id")
                if not nct_id:
                    continue
                
                # Ï§ëÎ≥µ Ï≤¥ÌÅ¨ (nct_id Í∏∞Ï§Ä)
                existing = supabase.table("golden_set_library")\
                    .select("id")\
                    .eq("properties->>nct_id", nct_id)\
                    .execute()
                
                if not existing.data:
                    supabase.table("golden_set_library").insert(entry).execute()
                    saved_count += 1
            except Exception as e:
                self.errors.append(f"Save error for {entry.get('properties', {}).get('nct_id')}: {str(e)[:100]}")
        
        return saved_count

    async def run_import(self, job_id: Optional[str] = None, max_studies: int = 5000):
        """
        Bulk Import Ïã§Ìñâ
        - JSON Îç§ÌîÑ Îã§Ïö¥Î°úÎìú (Ïä§Ìä∏Î¶¨Î∞ç)
        - ADC Í¥ÄÎ†® Îç∞Ïù¥ÌÑ∞ ÌïÑÌÑ∞ÎßÅ
        - ÏùºÍ¥Ñ DB Ï†ÅÏû¨
        """
        from app.api.scheduler import update_job_status, is_cancelled
        
        if job_id:
            await update_job_status(job_id, status="running")
        
        logger.info("üöÄ [Bulk Importer] Starting ClinicalTrials.gov dump download...")
        
        batch = []
        batch_size = 100
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            async with aiohttp.ClientSession(headers=headers) as session:
                logger.info(f"üì• Downloading from {DUMP_URL}...")
                
                async with session.get(DUMP_URL, timeout=aiohttp.ClientTimeout(total=3600)) as response:
                    if response.status != 200:
                        error_msg = f"Download failed: HTTP {response.status}"
                        logger.error(error_msg)
                        if job_id:
                            await update_job_status(job_id, status="failed", errors=[error_msg])
                        return
                    
                    # Ï†ÑÏ≤¥ ÏΩòÌÖêÏ∏† Îã§Ïö¥Î°úÎìú (ÎåÄÏö©ÎüâÏù¥ÎØÄÎ°ú ÏãúÍ∞Ñ ÏÜåÏöî)
                    logger.info("üì¶ Downloading ZIP file... (this may take several minutes)")
                    content = await response.read()
                    
                    if job_id:
                        await update_job_status(job_id, records_found=0)
                    
                    logger.info("üìÇ Extracting and parsing JSON files...")
                    
                    with zipfile.ZipFile(io.BytesIO(content)) as z:
                        json_files = [f for f in z.namelist() if f.endswith('.json')]
                        total_files = len(json_files)
                        logger.info(f"Found {total_files} JSON files in archive")
                        
                        for idx, filename in enumerate(json_files):
                            # Ï§ëÎã® ÏöîÏ≤≠ Ï≤¥ÌÅ¨
                            if job_id and await is_cancelled(job_id):
                                logger.info("Import cancelled by user")
                                await update_job_status(job_id, status="stopped")
                                return
                            
                            # ÏµúÎåÄ ÏàòÏßë Í∞úÏàò Ï≤¥ÌÅ¨
                            if self.total_imported >= max_studies:
                                logger.info(f"Reached max studies limit: {max_studies}")
                                break
                            
                            try:
                                with z.open(filename) as f:
                                    study_data = json.load(f)
                                    self.total_processed += 1
                                    
                                    # ADC Í¥ÄÎ†® Îç∞Ïù¥ÌÑ∞Îßå ÌïÑÌÑ∞ÎßÅ
                                    if self.is_adc_related(study_data):
                                        entry = self.extract_study_info(study_data)
                                        batch.append(entry)
                                        
                                        # Î∞∞ÏπòÍ∞Ä Ï∞®Î©¥ Ï†ÄÏû•
                                        if len(batch) >= batch_size:
                                            saved = await self.save_batch(batch, job_id)
                                            self.total_imported += saved
                                            batch = []
                                            
                                            if job_id:
                                                await update_job_status(
                                                    job_id, 
                                                    records_found=self.total_processed,
                                                    records_drafted=self.total_imported
                                                )
                                            
                                            logger.info(f"Progress: {self.total_processed}/{total_files} files, {self.total_imported} ADC studies imported")
                            
                            except json.JSONDecodeError:
                                continue
                            except Exception as e:
                                self.errors.append(str(e)[:100])
                    
                    # ÎÇ®ÏùÄ Î∞∞Ïπò Ï†ÄÏû•
                    if batch:
                        saved = await self.save_batch(batch, job_id)
                        self.total_imported += saved
            
            # ÏôÑÎ£å
            logger.info(f"üéâ Import Complete! Total: {self.total_imported} ADC studies from {self.total_processed} files")
            
            if job_id:
                await update_job_status(
                    job_id,
                    status="completed",
                    records_found=self.total_processed,
                    records_drafted=self.total_imported,
                    completed_at=datetime.utcnow().isoformat(),
                    errors=self.errors[:20]
                )
        
        except Exception as e:
            logger.error(f"Bulk Import Error: {e}")
            if job_id:
                await update_job_status(job_id, status="failed", errors=[str(e)])

# Ïã±Í∏ÄÌÜ§ Ïù∏Ïä§ÌÑ¥Ïä§
bulk_importer = BulkImporter()
