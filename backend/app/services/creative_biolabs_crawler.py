"""
Creative Biolabs ADC ì œí’ˆ í¬ë¡¤ëŸ¬
ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ë° í‚¤ì›Œë“œ ê²€ìƒ‰ ê¸°ë°˜ í¬ë¡¤ë§
"""
import aiohttp
import asyncio
from datetime import datetime
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
from app.core.supabase import supabase
import logging
import re

logger = logging.getLogger(__name__)

class CreativeBiolabsScraper:
    BASE_URL = "https://www.creative-biolabs.com"
    
    # ì‹¤ì œ ì œí’ˆ ì¹´í…Œê³ ë¦¬ URL (ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„ ê²°ê³¼)
    CATEGORY_URLS = [
        "https://www.creative-biolabs.com/adc/target-list-customized-adcs-1.htm",
        "https://www.creative-biolabs.com/adc/classify-druglnk-products-2.htm",
        "https://www.creative-biolabs.com/adc/target-list-anti-ab-adcs-3.htm",
        "https://www.creative-biolabs.com/adc/classify-anti-drug-abs-4.htm",
        "https://www.creative-biolabs.com/adc/target-list-customized-fluoroab-9.htm",
        "https://www.creative-biolabs.com/adc/classify-fluorescent-dyes-10.htm"
    ]
    
    # ADC ê´€ë ¨ ê²€ìƒ‰ í‚¤ì›Œë“œ
    SEARCH_TERMS = [
        "HER2", "TROP2", "EGFR", "CD19", "CD22", "CD33", 
        "MMAE", "DM1", "DXd", "vedotin", "trastuzumab"
    ]
    
    async def run(self, search_term: Optional[str] = None, max_pages: int = 5, job_id: Optional[str] = None):
        """
        í¬ë¡¤ë§ ì‹¤í–‰ (aiohttp & DB Status)
        """
        from app.api.scheduler import update_job_status, get_job_from_db
        
        if job_id:
            await update_job_status(job_id, status="running")

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5"
        }

        async with aiohttp.ClientSession(headers=headers) as session:
            try:
                if search_term:
                    # íŠ¹ì • í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
                    await self.run_search_mode(session, search_term, job_id)
                else:
                    # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ + í‚¤ì›Œë“œ ê²€ìƒ‰ ëª¨ë“œ
                    await self.run_full_crawl_mode(session, job_id)
                
                job = await get_job_from_db(job_id) if job_id else None
                if job and job["status"] != "stopped":
                    await update_job_status(job_id, status="completed", completed_at=datetime.utcnow().isoformat())
            except Exception as e:
                logger.error(f"Crawler Global Error: {e}")
                if job_id:
                    await update_job_status(job_id, status="failed", errors=[str(e)])

    async def run_search_mode(self, session: aiohttp.ClientSession, term: str, job_id: Optional[str] = None):
        """í‚¤ì›Œë“œ ê²€ìƒ‰ ëª¨ë“œ"""
        search_url = f"{self.BASE_URL}/adc/search.aspx?q={term}"
        logger.info(f"ğŸ” Searching for: {term}")
        await self.process_list_page(session, search_url, job_id)

    async def run_full_crawl_mode(self, session: aiohttp.ClientSession, job_id: Optional[str] = None):
        """ì „ì²´ í¬ë¡¤ë§ ëª¨ë“œ: ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ + í‚¤ì›Œë“œ ê²€ìƒ‰"""
        from app.api.scheduler import is_cancelled, update_job_status
        
        # 1. ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ í¬ë¡¤ë§
        logger.info("ğŸ“‚ Crawling category pages...")
        for cat_url in self.CATEGORY_URLS:
            if job_id and await is_cancelled(job_id):
                break
            await self.process_list_page(session, cat_url, job_id)
            await asyncio.sleep(1)  # Rate limiting
        
        # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ í¬ë¡¤ë§
        logger.info("ğŸ” Crawling by search keywords...")
        for term in self.SEARCH_TERMS:
            if job_id and await is_cancelled(job_id):
                break
            search_url = f"{self.BASE_URL}/adc/search.aspx?q={term}"
            await self.process_list_page(session, search_url, job_id)
            await asyncio.sleep(1)  # Rate limiting

    async def process_list_page(self, session: aiohttp.ClientSession, url: str, job_id: Optional[str] = None):
        """ëª©ë¡ í˜ì´ì§€ì—ì„œ ì œí’ˆ ë§í¬ ì¶”ì¶œ ë° ì²˜ë¦¬"""
        from app.api.scheduler import update_job_status, get_job_from_db, is_cancelled
        
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as res:
                if res.status != 200:
                    logger.warning(f"Page load failed: {url} -> {res.status}")
                    return
                    
                html = await res.text()
                soup = BeautifulSoup(html, "html.parser")
                
                # ì œí’ˆ ë§í¬ ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´)
                product_links = set()
                
                # íŒ¨í„´ 1: ì œí’ˆ ìƒì„¸ í˜ì´ì§€ ë§í¬
                for a in soup.select("a[href*='.htm']"):
                    href = a.get('href', '')
                    if '/adc/' in href and not any(x in href for x in ['search', 'home', 'contact', 'about']):
                        if not href.startswith("http"):
                            href = self.BASE_URL + href
                        product_links.add(href)
                
                # íŒ¨í„´ 2: ì œí’ˆ ì¹´ë“œ/ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ
                for a in soup.select(".product-item a, .item-box a, .product-list a"):
                    href = a.get('href', '')
                    if href and not href.startswith("http"):
                        href = self.BASE_URL + href
                    if href:
                        product_links.add(href)

                logger.info(f"Found {len(product_links)} product links on {url[:50]}...")

                if job_id:
                    job = await get_job_from_db(job_id)
                    current_found = job.get("records_found", 0) if job else 0
                    await update_job_status(job_id, records_found=current_found + len(product_links))

                # ë³‘ë ¬ ì²˜ë¦¬: 5ê°œì”©
                product_list = list(product_links)
                for i in range(0, len(product_list), 5):
                    if job_id and await is_cancelled(job_id):
                        break
                    batch = product_list[i:i+5]
                    tasks = [self.parse_product_page(session, link, job_id) for link in batch]
                    await asyncio.gather(*tasks, return_exceptions=True)
                    await asyncio.sleep(0.5)  # Rate limiting
                    
        except Exception as e:
            logger.error(f"List Page Error {url}: {e}")

    async def parse_product_page(self, session: aiohttp.ClientSession, url: str, job_id: Optional[str] = None):
        """ì œí’ˆ ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ë° ì €ì¥"""
        from app.api.scheduler import update_job_status, get_job_from_db
        
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=20)) as res:
                if res.status != 200:
                    return
                    
                html = await res.text()
                soup = BeautifulSoup(html, "html.parser")
                
                # ì œí’ˆëª… ì¶”ì¶œ
                name = "Unknown Product"
                for selector in ["h1", ".product-name", ".product-title", "title"]:
                    elem = soup.select_one(selector)
                    if elem:
                        name = elem.text.strip()
                        break
                
                # ì œí’ˆ ì„¤ëª… ì¶”ì¶œ
                description = ""
                for selector in [".product-description", ".description", ".content", "p"]:
                    elem = soup.select_one(selector)
                    if elem:
                        description = elem.text.strip()[:500]
                        break
                
                # ì¹´íƒˆë¡œê·¸ ë²ˆí˜¸ ì¶”ì¶œ
                catalog_match = re.search(r"(CAT|CB|BCA|ADC)[A-Z0-9\-]+", html)
                catalog_number = catalog_match.group(0) if catalog_match else None
                
                # SMILES ë³´ê°• (PubChem)
                smiles = None
                try:
                    from app.services.chemical_resolver import chemical_resolver
                    smiles = chemical_resolver.fetch_verified_smiles(name)
                except Exception:
                    pass
                
                # UniProt ID ì¶”ì¶œ
                uniprot_match = re.search(r"[OPQ][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z][0-9]([A-Z][A-Z0-9]{2}[0-9]){1,2}", html)
                uniprot_id = uniprot_match.group(0) if uniprot_match else None
                
                product_data = {
                    "name": name[:200],
                    "source_url": url,
                    "catalog_number": catalog_number,
                    "description": description,
                    "uniprot_id": uniprot_id,
                    "smiles_code": smiles,
                    "raw_data": {"html_title": soup.title.string if soup.title else None},
                    "source": "creative_biolabs",
                    "status": "available"
                }
                
                # commercial_reagents í…Œì´ë¸”ì— ì €ì¥ (upsert)
                supabase.table("commercial_reagents").upsert(product_data, on_conflict="name").execute()
                
                if job_id:
                    job = await get_job_from_db(job_id)
                    current_drafted = job.get("records_drafted", 0) if job else 0
                    await update_job_status(job_id, records_drafted=current_drafted + 1)
                
                logger.info(f"âœ… Saved: {name[:50]}...")
                    
        except Exception as e:
            logger.error(f"Product Page Error {url}: {e}")

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
creative_crawler = CreativeBiolabsScraper()
