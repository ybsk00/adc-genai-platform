import asyncio
import os
import sys
import logging
from dotenv import load_dotenv

# backend ê²½ë¡œ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv(os.path.join(current_dir, ".env"))

from app.services.ambeed_crawler import ambeed_crawler
from app.core.supabase import supabase

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("BulkScraper")

async def run_bulk():
    # ì‚¬ì¥ë‹˜ ì§€ì‹œì‚¬í•­ ë°˜ì˜
    # 1. 2ê°œì”© í•˜ì§€ ë§ê³  í¬ê²Œ ë¬¶ì–´ì„œ (batch_size=50)
    # 2. ìµœëŒ€í•œ ë§ì´ (limit=5000)
    # 3. 51í˜ì´ì§€ë¶€í„° (ê³¼ê±° ë°ì´í„° êµ¬ì—­)
    
    start_page = 51
    limit = 5000
    batch_size = 50
    job_id = "bulk_manual_run_001"
    
    print(f"\nğŸ”¥ [ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹œì‘] ì‹œì‘í˜ì´ì§€: {start_page}, ëª©í‘œëŸ‰: {limit}, ì €ì¥ë‹¨ìœ„: {batch_size}")
    
    try:
        # DB ì—°ê²° í™•ì¸
        res = supabase.table("commercial_reagents").select("count", count="exact").limit(1).execute()
        print(f"âœ… DB ì—°ê²° í™•ì¸ë¨. í˜„ì¬ ë°ì´í„° ìˆ˜: {res.count}")
        
        # í¬ë¡¤ëŸ¬ ì‹¤í–‰
        await ambeed_crawler.run(
            search_term="all", 
            limit=limit, 
            job_id=job_id, 
            start_page=start_page,
            batch_size=batch_size
        )
        
    except Exception as e:
        logger.error(f"âŒ ëŒ€ëŸ‰ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(run_bulk())
